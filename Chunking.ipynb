{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install PyMuPDF \n",
    "%pip install fitz\n",
    "%pip install tqdm\n",
    "%pip install bs4\n",
    "%pip install PyPDF2\n",
    "%pip install langchain\n",
    "%pip install nltk\n",
    "%pip install spacy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Ignore all warnings (not recommended unless you know the implications)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Filter out specific warnings\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import os\n",
    "import PyPDF2\n",
    "import re\n",
    "\n",
    "# URL of the webpage to scrape\n",
    "url = \"https://www.icar-crida.res.in/publications_annualreports.html\"\n",
    "\n",
    "def fetch_pdf_urls(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        content = BeautifulSoup(response.content, 'html.parser')\n",
    "        links = content.find_all('a')\n",
    "        pdf_links = []\n",
    "        for link in links:\n",
    "            href = link.get('href')\n",
    "            if href and href.endswith('.pdf'):\n",
    "                if not href.startswith('http'):\n",
    "                    href = 'https://www.icar-crida.res.in/' + href.lstrip('/')\n",
    "                # Adjusting to match only specific structure ending with .pdf directly after Annualreports/\n",
    "                if re.match(r'https://www\\.icar-crida\\.res\\.in/.*?/Annualreports/[^/]+\\.pdf$', href):\n",
    "                    pdf_links.append(href)\n",
    "        return pdf_links\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Failed to fetch webpage: {url} with error: {e}\")\n",
    "        return []\n",
    "\n",
    "def download_pdfs(pdf_urls, download_dir='downloaded_pdfs'):\n",
    "    os.makedirs(download_dir, exist_ok=True)\n",
    "    downloaded_files = []\n",
    "    for url in pdf_urls:\n",
    "        filename = url.split('/')[-1]\n",
    "        filepath = os.path.join(download_dir, filename)\n",
    "        if os.path.exists(filepath):\n",
    "            print(f\"Skipping {filename}. Already downloaded.\")\n",
    "            downloaded_files.append(filepath)\n",
    "            continue\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "            with open(filepath, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "            downloaded_files.append(filepath)\n",
    "            print(f\"Downloaded {filename} to {download_dir}\")\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Failed to download {filename}: {e}\")\n",
    "    return downloaded_files\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        text = ''\n",
    "        for page_num in range(len(reader.pages)):\n",
    "            page = reader.pages[page_num]\n",
    "            text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with a single space\n",
    "    return text.strip()\n",
    "\n",
    "# Fetch PDF URLs and download PDFs\n",
    "pdf_urls = fetch_pdf_urls(url)\n",
    "\n",
    "# Save PDF URLs to a JSON file\n",
    "with open('icar_crida_report_urls.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(pdf_urls, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(pdf_urls)\n",
    "downloaded_pdfs = download_pdfs(pdf_urls)\n",
    "print(\"PDF download and URL scraping completed!\")\n",
    "\n",
    "# Directory to save the text files\n",
    "text_dir = 'extracted_texts'\n",
    "os.makedirs(text_dir, exist_ok=True)\n",
    "\n",
    "# Process each downloaded PDF\n",
    "for pdf_path in downloaded_pdfs:\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    cleaned_text = clean_text(text)\n",
    "    txt_filename = os.path.basename(pdf_path).replace('.pdf', '.txt')\n",
    "    txt_filepath = os.path.join(text_dir, txt_filename)\n",
    "    with open(txt_filepath, 'w', encoding='utf-8') as file:\n",
    "        file.write(cleaned_text)\n",
    "    print(f\"Extracted, cleaned, and saved text from {pdf_path} to {txt_filepath}\")\n",
    "\n",
    "print(\"Text extraction, cleaning, and saving completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "# Function to extract text from PDF\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        text = \"\"\n",
    "        for pgno in range(len(doc)):\n",
    "            page = doc.load_page(pgno)\n",
    "            text += page.get_text().replace('\\n', ' ')  # Remove \\n characters\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {pdf_path}: {e}\")\n",
    "        return \"\"\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "# Function to preprocess text and chunk into sentences with overlap\n",
    "def preprocess_and_chunk_sentences(text, max_chunk_size=300, chunk_overlap=10):\n",
    "    sentences = sent_tokenize(text)  # Tokenize text into sentences\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        tokens = word_tokenize(sentence)  # Tokenize each sentence into words\n",
    "        for token in tokens:\n",
    "            current_chunk.append(token)\n",
    "            if len(' '.join(current_chunk)) >= max_chunk_size:\n",
    "                # Create the chunk and add it to the list of chunks\n",
    "                chunks.append(' '.join(current_chunk))\n",
    "                # Create the overlap for the next chunk\n",
    "                current_chunk = current_chunk[-chunk_overlap:]\n",
    "    \n",
    "    # Add any remaining tokens as the last chunk\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "\n",
    "# Function to process PDFs and directly create text files\n",
    "def process_pdfs_and_create_files(pdf_dir='downloaded_pdfs', output_folder='final_chunks'):\n",
    "    os.makedirs(output_folder, exist_ok=True)  # Ensure the output folder exists\n",
    "\n",
    "    for filename in tqdm(os.listdir(pdf_dir), desc=\"Processing PDFs\"):\n",
    "        if filename.endswith('.pdf'):\n",
    "            pdf_path = os.path.join(pdf_dir, filename)\n",
    "            text = extract_text_from_pdf(pdf_path)\n",
    "            if text:\n",
    "                # Preprocess and chunk text into sentences\n",
    "                chunks = preprocess_and_chunk_sentences(text)\n",
    "                # Create text file for each PDF\n",
    "                output_file_path = os.path.join(output_folder, f\"{os.path.splitext(filename)[0]}.txt\")\n",
    "                with open(output_file_path, 'w', encoding='utf-8') as f:\n",
    "                    for chunk in chunks:\n",
    "                        f.write(chunk + '\\n\\n')  # Add new lines between chunks\n",
    "                print(f\"Processed {filename}: {len(chunks)} chunks\")\n",
    "\n",
    "# Process PDFs and directly create text files\n",
    "process_pdfs_and_create_files()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
