{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache directory already exists: D:\\huggingface_cache\n",
      "Default cache directory does not exist: C:\\Users\\KARTHIK/.cache/huggingface/transformers\n",
      "TRANSFORMERS_CACHE is set to: D:\\huggingface_cache\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Set the TRANSFORMERS_CACHE environment variable to a directory on your D drive\n",
    "os.environ['TRANSFORMERS_CACHE'] = 'D:\\\\huggingface_cache'\n",
    "os.environ['HF_HOME'] = 'D:/huggingface_cache'\n",
    "\n",
    "# Ensure the cache directory exists\n",
    "cache_dir = 'D:\\\\huggingface_cache'\n",
    "if not os.path.exists(cache_dir):\n",
    "    os.makedirs(cache_dir)\n",
    "    print(f\"Created cache directory: {cache_dir}\")\n",
    "else:\n",
    "    print(f\"Cache directory already exists: {cache_dir}\")\n",
    "\n",
    "# Optional: Clear existing cache in the default location\n",
    "default_cache_dir = os.path.expanduser('~/.cache/huggingface/transformers')\n",
    "if os.path.exists(default_cache_dir):\n",
    "    shutil.rmtree(default_cache_dir)\n",
    "    print(f\"Deleted cache directory: {default_cache_dir}\")\n",
    "else:\n",
    "    print(f\"Default cache directory does not exist: {default_cache_dir}\")\n",
    "\n",
    "# Verify environment variable\n",
    "print(f\"TRANSFORMERS_CACHE is set to: {os.getenv('TRANSFORMERS_CACHE')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyMuPDF in d:\\chunks_of_different_types\\.venv\\lib\\site-packages (1.24.7)\n",
      "Requirement already satisfied: PyMuPDFb==1.24.6 in d:\\chunks_of_different_types\\.venv\\lib\\site-packages (from PyMuPDF) (1.24.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting fitzNote: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Building wheel for traits (pyproject.toml) did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [286 lines of output]\n",
      "      Reading version file C:\\Users\\KARTHIK\\AppData\\Local\\Temp\\pip-install-l19938t3\\traits_3aa8aacfb8924fd7967bfa40e7fa1581\\traits\\version.py\n",
      "      Package version from version file: ('6.3.2', '6df2ff9bd1d21c74e688aff6f67a19fbbefdd53b')\n",
      "      running bdist_wheel\n",
      "      running build\n",
      "      running build_py\n",
      "      creating build\n",
      "      creating build\\lib.win-amd64-cpython-311\n",
      "      creating build\\lib.win-amd64-cpython-311\\traits\n",
      "      copying traits\\api.py -> build\\lib.win-amd64-cpython-311\\traits\n",
      "      copying traits\\base_trait_handler.py -> build\\lib.win-amd64-cpython-311\\traits\n",
      "      copying traits\\constants.py -> build\\lib.win-amd64-cpython-311\\traits\n",
      "      copying traits\\ctrait.py -> build\\lib.win-amd64-cpython-311\\traits\n",
      "      copying traits\\editor_factories.py -> build\\lib.win-amd64-cpython-311\\traits\n",
      "      copying traits\\has_traits.py -> build\\lib.win-amd64-cpython-311\\traits\n",
      "      copying traits\\interface_checker.py -> build\\lib.win-amd64-cpython-311\\traits\n",
      "      copying traits\\traits.py -> build\\lib.win-amd64-cpython-311\\traits\n",
      "      copying traits\\traits_listener.py -> build\\lib.win-amd64-cpython-311\\traits\n",
      "      copying traits\\trait_base.py -> build\\lib.win-amd64-cpython-311\\traits\n",
      "      copying traits\\trait_converters.py -> build\\lib.win-amd64-cpython-311\\traits\n",
      "      copying traits\\trait_dict_object.py -> build\\lib.win-amd64-cpython-311\\traits\n",
      "      copying traits\\trait_errors.py -> build\\lib.win-amd64-cpython-311\\traits\n",
      "      copying traits\\trait_factory.py -> build\\lib.win-amd64-cpython-311\\traits\n",
      "      copying traits\\trait_handler.py -> build\\lib.win-amd64-cpython-311\\traits\n",
      "      copying traits\\trait_handlers.py -> build\\lib.win-amd64-cpython-311\\traits\n",
      "      copying traits\\trait_list_object.py -> build\\lib.win-amd64-cpython-311\\traits\n",
      "      copying traits\\trait_notifiers.py -> build\\lib.win-amd64-cpython-311\\traits\n",
      "      copying traits\\trait_numeric.py -> build\\lib.win-amd64-cpython-311\\traits\n",
      "      copying traits\\trait_set_object.py -> build\\lib.win-amd64-cpython-311\\traits\n",
      "      copying traits\\trait_type.py -> build\\lib.win-amd64-cpython-311\\traits\n",
      "      copying traits\\trait_types.py -> build\\lib.win-amd64-cpython-311\\traits\n",
      "      copying traits\\version.py -> build\\lib.win-amd64-cpython-311\\traits\n",
      "      copying traits\\__init__.py -> build\\lib.win-amd64-cpython-311\\traits\n",
      "      creating build\\lib.win-amd64-cpython-311\\traits\\adaptation\n",
      "      copying traits\\adaptation\\adaptation_error.py -> build\\lib.win-amd64-cpython-311\\traits\\adaptation\n",
      "      copying traits\\adaptation\\adaptation_manager.py -> build\\lib.win-amd64-cpython-311\\traits\\adaptation\n",
      "      copying traits\\adaptation\\adaptation_offer.py -> build\\lib.win-amd64-cpython-311\\traits\\adaptation\n",
      "      copying traits\\adaptation\\adapter.py -> build\\lib.win-amd64-cpython-311\\traits\\adaptation\n",
      "      copying traits\\adaptation\\api.py -> build\\lib.win-amd64-cpython-311\\traits\\adaptation\n",
      "      copying traits\\adaptation\\__init__.py -> build\\lib.win-amd64-cpython-311\\traits\\adaptation\n",
      "      creating build\\lib.win-amd64-cpython-311\\traits\\etsconfig\n",
      "      copying traits\\etsconfig\\api.py -> build\\lib.win-amd64-cpython-311\\traits\\etsconfig\n",
      "      copying traits\\etsconfig\\etsconfig.py -> build\\lib.win-amd64-cpython-311\\traits\\etsconfig\n",
      "      copying traits\\etsconfig\\__init__.py -> build\\lib.win-amd64-cpython-311\\traits\\etsconfig\n",
      "      creating build\\lib.win-amd64-cpython-311\\traits\\examples\n",
      "      copying traits\\examples\\_etsdemo_info.py -> build\\lib.win-amd64-cpython-311\\traits\\examples\n",
      "      copying traits\\examples\\__init__.py -> build\\lib.win-amd64-cpython-311\\traits\\examples\n",
      "      creating build\\lib.win-amd64-cpython-311\\traits\\observation\n",
      "      copying traits\\observation\\api.py -> build\\lib.win-amd64-cpython-311\\traits\\observation\n",
      "      copying traits\\observation\\events.py -> build\\lib.win-amd64-cpython-311\\traits\\observation\n",
      "      copying traits\\observation\\exceptions.py -> build\\lib.win-amd64-cpython-311\\traits\\observation\n",
      "      copying traits\\observation\\exception_handling.py -> build\\lib.win-amd64-cpython-311\\traits\\observation\n",
      "      copying traits\\observation\\expression.py -> build\\lib.win-amd64-cpython-311\\traits\\observation\n",
      "      copying traits\\observation\\i_observable.py -> build\\lib.win-amd64-cpython-311\\traits\\observation\n",
      "      copying traits\\observation\\observe.py -> build\\lib.win-amd64-cpython-311\\traits\\observation\n",
      "      copying traits\\observation\\parsing.py -> build\\lib.win-amd64-cpython-311\\traits\\observation\n",
      "      copying traits\\observation\\_anytrait_filter.py -> build\\lib.win-amd64-cpython-311\\traits\\observation\n",
      "      copying traits\\observation\\_dict_change_event.py -> build\\lib.win-amd64-cpython-311\\traits\\observation\n",
      "      copying traits\\observation\\_dict_item_observer.py -> build\\lib.win-amd64-cpython-311\\traits\\observation\n",
      "      copying traits\\observation\\_filtered_trait_observer.py -> build\\lib.win-amd64-cpython-311\\traits\\observation\n",
      "      copying traits\\observation\\_generated_parser.py -> build\\lib.win-amd64-cpython-311\\traits\\observation\n",
      "      copying traits\\observation\\_has_traits_helpers.py -> build\\lib.win-amd64-cpython-311\\traits\\observation\n",
      "      copying traits\\observation\\_i_notifier.py -> build\\lib.win-amd64-cpython-311\\traits\\observation\n",
      "      copying traits\\observation\\_i_observer.py -> build\\lib.win-amd64-cpython-311\\traits\\observation\n",
      "      copying traits\\observation\\_list_change_event.py -> build\\lib.win-amd64-cpython-311\\traits\\observation\n",
      "      copying traits\\observation\\_list_item_observer.py -> build\\lib.win-amd64-cpython-311\\traits\\observation\n",
      "      copying traits\\observation\\_metadata_filter.py -> build\\lib.win-amd64-cpython-311\\traits\\observation\n",
      "      copying traits\\observation\\_named_trait_observer.py -> build\\lib.win-amd64-cpython-311\\traits\\observation\n",
      "      copying traits\\observation\\_observe.py -> build\\lib.win-amd64-cpython-311\\traits\\observation\n",
      "      copying traits\\observation\\_observer_change_notifier.py -> build\\lib.win-amd64-cpython-311\\traits\\observation\n",
      "      copying traits\\observation\\_observer_graph.py -> build\\lib.win-amd64-cpython-311\\traits\\observation\n",
      "      copying traits\\observation\\_set_change_event.py -> build\\lib.win-amd64-cpython-311\\traits\\observation\n",
      "      copying traits\\observation\\_set_item_observer.py -> build\\lib.win-amd64-cpython-311\\traits\\observation\n",
      "      copying traits\\observation\\_testing.py -> build\\lib.win-amd64-cpython-311\\traits\\observation\n",
      "      copying traits\\observation\\_trait_added_observer.py -> build\\lib.win-amd64-cpython-311\\traits\\observation\n",
      "      copying traits\\observation\\_trait_change_event.py -> build\\lib.win-amd64-cpython-311\\traits\\observation\n",
      "      copying traits\\observation\\_trait_event_notifier.py -> build\\lib.win-amd64-cpython-311\\traits\\observation\n",
      "      copying traits\\observation\\__init__.py -> build\\lib.win-amd64-cpython-311\\traits\\observation\n",
      "      creating build\\lib.win-amd64-cpython-311\\traits\\testing\n",
      "      copying traits\\testing\\api.py -> build\\lib.win-amd64-cpython-311\\traits\\testing\n",
      "      copying traits\\testing\\doctest_tools.py -> build\\lib.win-amd64-cpython-311\\traits\\testing\n",
      "      copying traits\\testing\\nose_tools.py -> build\\lib.win-amd64-cpython-311\\traits\\testing\n",
      "      copying traits\\testing\\optional_dependencies.py -> build\\lib.win-amd64-cpython-311\\traits\\testing\n",
      "      copying traits\\testing\\unittest_tools.py -> build\\lib.win-amd64-cpython-311\\traits\\testing\n",
      "      copying traits\\testing\\__init__.py -> build\\lib.win-amd64-cpython-311\\traits\\testing\n",
      "      creating build\\lib.win-amd64-cpython-311\\traits\\tests\n",
      "      copying traits\\tests\\check_observe_timing.py -> build\\lib.win-amd64-cpython-311\\traits\\tests\n",
      "      copying traits\\tests\\check_timing.py -> build\\lib.win-amd64-cpython-311\\traits\\tests\n",
      "      copying traits\\tests\\test_abc.py -> build\\lib.win-amd64-cpython-311\\traits\\tests\n",
      "      copying traits\\tests\\test_any.py -> build\\lib.win-amd64-cpython-311\\traits\\tests\n",
      "      copying traits\\tests\\test_anytrait_static_notifiers.py -> build\\lib.win-amd64-cpython-311\\traits\\tests\n",
      "      copying traits\\tests\\test_array.py -> build\\lib.win-amd64-cpython-311\\traits\\tests\n",
      "      copying traits\\tests\\test_array_or_none.py -> build\\lib.win-amd64-cpython-311\\traits\\tests\n",
      "      copying traits\\tests\\test_automatic_adaptation.py -> build\\lib.win-amd64-cpython-311\\traits\\tests\n",
      "      copying traits\\tests\\test_bool.py -> build\\lib.win-amd64-cpython-311\\traits\\tests\n",
      "      copying traits\\tests\\test_callable.py -> build\\lib.win-amd64-cpython-311\\traits\\tests\n",
      "      copying traits\\tests\\test_class_traits.py -> build\\lib.win-amd64-cpython-311\\traits\\tests\n",
      "      copying traits\\tests\\test_clone.py -> build\\lib.win-amd64-cpython-311\\traits\\tests\n",
      "      copying traits\\tests\\test_configure_traits.py -> build\\lib.win-amd64-cpython-311\\traits\\tests\n",
      "      copying traits\\tests\\test_constant.py -> build\\lib.win-amd64-cpython-311\\traits\\tests\n",
      "      copying traits\\tests\\test_constants.py -> build\\lib.win-amd64-cpython-311\\traits\\tests\n",
      "      copying traits\\tests\\test_container_events.py -> build\\lib.win-amd64-cpython-311\\traits\\tests\n",
      "      copying traits\\tests\\test_copyable_trait_names.py -> build\\lib.win-amd64-cpython-311\\traits\\tests\n",
      "      copying traits\\tests\\test_copy_traits.py -> build\\lib.win-amd64-cpython-311\\traits\\tests\n",
      "      copying traits\\tests\\test_ctraits.py -> build\\lib.win-amd64-cpython-311\\traits\\tests\n",
      "      copying traits\\tests\\test_cythonized_traits.py -> build\\lib.win-amd64-cpython-311\\traits\\tests\n",
      "      copying traits\\tests\\test_date.py -> build\\lib.win-amd64-cpython-311\\traits\\tests\n",
      "      copying traits\\tests\\test_datetime.py -> build\\lib.win-amd64-cpython-311\\traits\\tests\n",
      "      copying traits\\tests\\test_delegate.py -> build\\lib.win-amd64-cpython-311\\traits\\tests\n",
      "      copying traits\\tests\\test_deprecated_handlers.py -> build\\lib.win-amd64-cpython-311\\traits\\tests\n",
      "      copying traits\\tests\\test_dict.py -> build\\lib.win-amd64-cpython-311\\traits\\tests\n",
      "      copying traits\\tests\\test_directory.py -> build\\lib.win-amd64-cpython-311\\traits\\tests\n",
      "      copying traits\\tests\\test_dynamic_notifiers.py -> build\\lib.win-amd64-cpython-311\\traits\\tests\n",
      "      copying traits\\tests\\test_dynamic_trait_definition.py -> build\\lib.win-amd64-cpython-311\\traits\\tests\n",
      "      copying traits\\tests\\test_editor_factories.py -> build\\lib.win-amd64-cpython-311\\traits\\tests\n",
      "      copying traits\\tests\\test_enum.py -> build\\lib.win-amd64-cpython-311\\traits\\tests\n",
      "      copying traits\\tests\\test_event_order.py -> build\\lib.win-amd64-cpython-311\\traits\\tests\n",
      "      copying traits\\tests\\test_expression.py -> build\\lib.win-amd64-cpython-311\\traits\\tests\n",
      "      copying traits\\tests\\test_extended_notifiers.py -> build\\lib.win-amd64-cpython-311\\traits\\tests\n",
      "      copying traits\\tests\\test_extended_trait_change.py -> build\\lib.win-amd64-cpython-311\\traits\\tests\n",
      "      copying traits\\tests\\test_file.py -> build\\lib.win-amd64-cpython-311\\traits\\tests\n",
      "      copying traits\\tests\\test_float.py -> build\\lib.win-amd64-cpython-311\\traits\\tests\n",
      "      copying traits\\tests\\test_float_range.py -> build\\lib.win-amd64-cpython-311\\traits\\tests\n",
      "      copying traits\\tests\\test_get_traits.py -> build\\lib.win-amd64-cpython-311\\traits\\tests\n",
      "      copying traits\\tests\\test_has_required_traits.py -> build\\lib.win-amd64-cpython-311\\traits\\tests\n",
      "      copying traits\\tests\\test_has_traits.py -> build\\lib.win-amd64-cpython-311\\traits\\tests\n",
      "      copying traits\\tests\\test_historical_unpickling.py -> build\\lib.win-amd64-cpython-311\\traits\\tests\n",
      "      copying traits\\tests\\test_instance.py -> build\\lib.win-amd64-cpython-311\\traits\\tests\n",
      "      copying traits\\tests\\test_integer.py -> build\\lib.win-amd64-cpython-311\\traits\\tests\n",
      "      copying traits\\tests\\test_integer_range.py -> build\\lib.win-amd64-cpython-311\\traits\\tests\n",
      "      copying traits\\tests\\test_interfaces.py -> build\\lib.win-amd64-cpython-311\\traits\\tests\n",
      "      copying traits\\tests\\test_interface_checker.py -> build\\lib.win-amd64-cpython-311\\traits\\tests\n",
      "      copying traits\\tests\\test_int_range_long.py -> build\\lib.win-amd64-cpython-311\\traits\\tests\n",
      "      copying traits\\tests\\test_keyword_args.py -> build\\lib.win-amd64-cpython-311\\traits\\tests\n",
      "      copying traits\\tests\\test_list.py -> build\\lib.win-amd64-cpython-311\\traits\\tests\n",
      "      copying traits\\tests\\test_listeners.py -> build\\lib.win-amd64-cpython-311\\traits\\tests\n",
      "      copying traits\\tests\\test_list_events.py -> build\\lib.win-amd64-cpython-311\\traits\\tests\n",
      "      copying traits\\tests\\test_long_traits.py -> build\\lib.win-amd64-cpython-311\\traits\\tests\n",
      "      copying traits\\tests\\test_map.py -> build\\lib.win-amd64-cpython-311\\traits\\tests\n",
      "      copying traits\\tests\\test_new_notifiers.py -> build\\lib.win-amd64-cpython-311\\traits\\tests\n",
      "      copying traits\\tests\\test_none.py -> build\\lib.win-amd64-cpython-311\\traits\\tests\n",
      "      copying traits\\tests\\test_observe.py -> build\\lib.win-amd64-cpython-311\\traits\\tests\n",
      "      copying traits\\tests\\test_pickle_validated_dict.py -> build\\lib.win-amd64-cpython-311\\traits\\tests\n",
      "      copying traits\\tests\\test_prefix_list.py -> build\\lib.win-amd64-cpython-311\\traits\\tests\n",
      "      copying traits\\tests\\test_prefix_map.py -> build\\lib.win-amd64-cpython-311\\traits\\tests\n",
      "      copying traits\\tests\\test_property_delete.py -> build\\lib.win-amd64-cpython-311\\traits\\tests\n",
      "      copying traits\\tests\\test_property_notifications.py -> build\\lib.win-amd64-cpython-311\\traits\\tests\n",
      "      copying traits\\tests\\test_python_properties.py -> build\\lib.win-amd64-cpython-311\\traits\\tests\n",
      "      copying traits\\tests\\test_range.py -> build\\lib.win-amd64-cpython-311\\traits\\tests\n",
      "      copying traits\\tests\\test_readonly.py -> build\\lib.win-amd64-cpython-311\\traits\\tests\n",
      "      copying traits\\tests\\test_regression.py -> build\\lib.win-amd64-cpython-311\\traits\\tests\n",
      "      copying traits\\tests\\test_rich_compare.py -> build\\lib.win-amd64-cpython-311\\traits\\tests\n",
      "      copying traits\\tests\\test_special_event_handlers.py -> build\\lib.win-amd64-cpython-311\\traits\\tests\n",
      "      copying traits\\tests\\test_static_notifiers.py -> build\\lib.win-amd64-cpython-311\\traits\\tests\n",
      "      copying traits\\tests\\test_string.py -> build\\lib.win-amd64-cpython-311\\traits\\tests\n",
      "      copying traits\\tests\\test_str_handler.py -> build\\lib.win-amd64-cpython-311\\traits\\tests\n",
      "      copying traits\\tests\\test_sync_traits.py -> build\\lib.win-amd64-cpython-311\\traits\\tests\n",
      "      copying traits\\tests\\test_target.py -> build\\lib.win-amd64-cpython-311\\traits\\tests\n",
      "      copying traits\\tests\\test_time.py -> build\\lib.win-amd64-cpython-311\\traits\\tests\n",
      "      copying traits\\tests\\test_traits.py -> build\\lib.win-amd64-cpython-311\\traits\\tests\n",
      "      copying traits\\tests\\test_traits_listener.py -> build\\lib.win-amd64-cpython-311\\traits\\tests\n",
      "      copying traits\\tests\\test_trait_base.py -> build\\lib.win-amd64-cpython-311\\traits\\tests\n",
      "      copying traits\\tests\\test_trait_change_event_tracer.py -> build\\lib.win-amd64-cpython-311\\traits\\tests\n",
      "      copying traits\\tests\\test_trait_converters.py -> build\\lib.win-amd64-cpython-311\\traits\\tests\n",
      "      copying traits\\tests\\test_trait_cycle.py -> build\\lib.win-amd64-cpython-311\\traits\\tests\n",
      "      copying traits\\tests\\test_trait_default_initializer.py -> build\\lib.win-amd64-cpython-311\\traits\\tests\n",
      "      copying traits\\tests\\test_trait_dict_list_set_event.py -> build\\lib.win-amd64-cpython-311\\traits\\tests\n",
      "      copying traits\\tests\\test_trait_dict_object.py -> build\\lib.win-amd64-cpython-311\\traits\\tests\n",
      "      copying traits\\tests\\test_trait_exceptions.py -> build\\lib.win-amd64-cpython-311\\traits\\tests\n",
      "      copying traits\\tests\\test_trait_get_set.py -> build\\lib.win-amd64-cpython-311\\traits\\tests\n",
      "      copying traits\\tests\\test_trait_list_dict.py -> build\\lib.win-amd64-cpython-311\\traits\\tests\n",
      "      copying traits\\tests\\test_trait_list_object.py -> build\\lib.win-amd64-cpython-311\\traits\\tests\n",
      "      copying traits\\tests\\test_trait_prefix_list.py -> build\\lib.win-amd64-cpython-311\\traits\\tests\n",
      "      copying traits\\tests\\test_trait_set_object.py -> build\\lib.win-amd64-cpython-311\\traits\\tests\n",
      "      copying traits\\tests\\test_trait_types.py -> build\\lib.win-amd64-cpython-311\\traits\\tests\n",
      "      copying traits\\tests\\test_tuple.py -> build\\lib.win-amd64-cpython-311\\traits\\tests\n",
      "      copying traits\\tests\\test_type.py -> build\\lib.win-amd64-cpython-311\\traits\\tests\n",
      "      copying traits\\tests\\test_ui_notifiers.py -> build\\lib.win-amd64-cpython-311\\traits\\tests\n",
      "      copying traits\\tests\\test_undefined.py -> build\\lib.win-amd64-cpython-311\\traits\\tests\n",
      "      copying traits\\tests\\test_unicode_traits.py -> build\\lib.win-amd64-cpython-311\\traits\\tests\n",
      "      copying traits\\tests\\test_union.py -> build\\lib.win-amd64-cpython-311\\traits\\tests\n",
      "      copying traits\\tests\\test_uuid.py -> build\\lib.win-amd64-cpython-311\\traits\\tests\n",
      "      copying traits\\tests\\test_validated_tuple.py -> build\\lib.win-amd64-cpython-311\\traits\\tests\n",
      "      copying traits\\tests\\test_version.py -> build\\lib.win-amd64-cpython-311\\traits\\tests\n",
      "      copying traits\\tests\\test_view_elements.py -> build\\lib.win-amd64-cpython-311\\traits\\tests\n",
      "      copying traits\\tests\\test_weak_ref.py -> build\\lib.win-amd64-cpython-311\\traits\\tests\n",
      "      copying traits\\tests\\tuple_test_mixin.py -> build\\lib.win-amd64-cpython-311\\traits\\tests\n",
      "      copying traits\\tests\\__init__.py -> build\\lib.win-amd64-cpython-311\\traits\\tests\n",
      "      creating build\\lib.win-amd64-cpython-311\\traits\\util\n",
      "      copying traits\\util\\api.py -> build\\lib.win-amd64-cpython-311\\traits\\util\n",
      "      copying traits\\util\\async_trait_wait.py -> build\\lib.win-amd64-cpython-311\\traits\\util\n",
      "      copying traits\\util\\camel_case.py -> build\\lib.win-amd64-cpython-311\\traits\\util\n",
      "      copying traits\\util\\clean_strings.py -> build\\lib.win-amd64-cpython-311\\traits\\util\n",
      "      copying traits\\util\\deprecated.py -> build\\lib.win-amd64-cpython-311\\traits\\util\n",
      "      copying traits\\util\\event_tracer.py -> build\\lib.win-amd64-cpython-311\\traits\\util\n",
      "      copying traits\\util\\home_directory.py -> build\\lib.win-amd64-cpython-311\\traits\\util\n",
      "      copying traits\\util\\import_symbol.py -> build\\lib.win-amd64-cpython-311\\traits\\util\n",
      "      copying traits\\util\\resource.py -> build\\lib.win-amd64-cpython-311\\traits\\util\n",
      "      copying traits\\util\\toposort.py -> build\\lib.win-amd64-cpython-311\\traits\\util\n",
      "      copying traits\\util\\trait_documenter.py -> build\\lib.win-amd64-cpython-311\\traits\\util\n",
      "      copying traits\\util\\weakiddict.py -> build\\lib.win-amd64-cpython-311\\traits\\util\n",
      "      copying traits\\util\\_traitsui_helpers.py -> build\\lib.win-amd64-cpython-311\\traits\\util\n",
      "      copying traits\\util\\__init__.py -> build\\lib.win-amd64-cpython-311\\traits\\util\n",
      "      creating build\\lib.win-amd64-cpython-311\\traits\\adaptation\\tests\n",
      "      copying traits\\adaptation\\tests\\abc_examples.py -> build\\lib.win-amd64-cpython-311\\traits\\adaptation\\tests\n",
      "      copying traits\\adaptation\\tests\\benchmark.py -> build\\lib.win-amd64-cpython-311\\traits\\adaptation\\tests\n",
      "      copying traits\\adaptation\\tests\\interface_examples.py -> build\\lib.win-amd64-cpython-311\\traits\\adaptation\\tests\n",
      "      copying traits\\adaptation\\tests\\lazy_examples.py -> build\\lib.win-amd64-cpython-311\\traits\\adaptation\\tests\n",
      "      copying traits\\adaptation\\tests\\test_adaptation_manager.py -> build\\lib.win-amd64-cpython-311\\traits\\adaptation\\tests\n",
      "      copying traits\\adaptation\\tests\\test_adaptation_offer.py -> build\\lib.win-amd64-cpython-311\\traits\\adaptation\\tests\n",
      "      copying traits\\adaptation\\tests\\test_adapter.py -> build\\lib.win-amd64-cpython-311\\traits\\adaptation\\tests\n",
      "      copying traits\\adaptation\\tests\\test_global_adaptation_manager.py -> build\\lib.win-amd64-cpython-311\\traits\\adaptation\\tests\n",
      "      copying traits\\adaptation\\tests\\__init__.py -> build\\lib.win-amd64-cpython-311\\traits\\adaptation\\tests\n",
      "      creating build\\lib.win-amd64-cpython-311\\traits\\etsconfig\\tests\n",
      "      copying traits\\etsconfig\\tests\\test_etsconfig.py -> build\\lib.win-amd64-cpython-311\\traits\\etsconfig\\tests\n",
      "      copying traits\\etsconfig\\tests\\__init__.py -> build\\lib.win-amd64-cpython-311\\traits\\etsconfig\\tests\n",
      "      creating build\\lib.win-amd64-cpython-311\\traits\\examples\\tests\n",
      "      copying traits\\examples\\tests\\test_etsdemo_info.py -> build\\lib.win-amd64-cpython-311\\traits\\examples\\tests\n",
      "      copying traits\\examples\\tests\\__init__.py -> build\\lib.win-amd64-cpython-311\\traits\\examples\\tests\n",
      "      creating build\\lib.win-amd64-cpython-311\\traits\\observation\\tests\n",
      "      copying traits\\observation\\tests\\test_dict_change_event.py -> build\\lib.win-amd64-cpython-311\\traits\\observation\\tests\n",
      "      copying traits\\observation\\tests\\test_dict_item_observer.py -> build\\lib.win-amd64-cpython-311\\traits\\observation\\tests\n",
      "      copying traits\\observation\\tests\\test_exception_handling.py -> build\\lib.win-amd64-cpython-311\\traits\\observation\\tests\n",
      "      copying traits\\observation\\tests\\test_expression.py -> build\\lib.win-amd64-cpython-311\\traits\\observation\\tests\n",
      "      copying traits\\observation\\tests\\test_filtered_trait_observer.py -> build\\lib.win-amd64-cpython-311\\traits\\observation\\tests\n",
      "      copying traits\\observation\\tests\\test_generated_parser.py -> build\\lib.win-amd64-cpython-311\\traits\\observation\\tests\n",
      "      copying traits\\observation\\tests\\test_has_traits_helpers.py -> build\\lib.win-amd64-cpython-311\\traits\\observation\\tests\n",
      "      copying traits\\observation\\tests\\test_list_change_event.py -> build\\lib.win-amd64-cpython-311\\traits\\observation\\tests\n",
      "      copying traits\\observation\\tests\\test_list_item_observer.py -> build\\lib.win-amd64-cpython-311\\traits\\observation\\tests\n",
      "      copying traits\\observation\\tests\\test_metadata_filter.py -> build\\lib.win-amd64-cpython-311\\traits\\observation\\tests\n",
      "      copying traits\\observation\\tests\\test_named_trait_observer.py -> build\\lib.win-amd64-cpython-311\\traits\\observation\\tests\n",
      "      copying traits\\observation\\tests\\test_observe.py -> build\\lib.win-amd64-cpython-311\\traits\\observation\\tests\n",
      "      copying traits\\observation\\tests\\test_observer_change_notifier.py -> build\\lib.win-amd64-cpython-311\\traits\\observation\\tests\n",
      "      copying traits\\observation\\tests\\test_observer_graph.py -> build\\lib.win-amd64-cpython-311\\traits\\observation\\tests\n",
      "      copying traits\\observation\\tests\\test_parsing.py -> build\\lib.win-amd64-cpython-311\\traits\\observation\\tests\n",
      "      copying traits\\observation\\tests\\test_set_change_event.py -> build\\lib.win-amd64-cpython-311\\traits\\observation\\tests\n",
      "      copying traits\\observation\\tests\\test_set_item_observer.py -> build\\lib.win-amd64-cpython-311\\traits\\observation\\tests\n",
      "      copying traits\\observation\\tests\\test_trait_added_observer.py -> build\\lib.win-amd64-cpython-311\\traits\\observation\\tests\n",
      "      copying traits\\observation\\tests\\test_trait_change_event.py -> build\\lib.win-amd64-cpython-311\\traits\\observation\\tests\n",
      "      copying traits\\observation\\tests\\test_trait_event_notifier.py -> build\\lib.win-amd64-cpython-311\\traits\\observation\\tests\n",
      "      copying traits\\observation\\tests\\__init__.py -> build\\lib.win-amd64-cpython-311\\traits\\observation\\tests\n",
      "      creating build\\lib.win-amd64-cpython-311\\traits\\testing\\tests\n",
      "      copying traits\\testing\\tests\\test_nose_tools.py -> build\\lib.win-amd64-cpython-311\\traits\\testing\\tests\n",
      "      copying traits\\testing\\tests\\test_optional_dependencies.py -> build\\lib.win-amd64-cpython-311\\traits\\testing\\tests\n",
      "      copying traits\\testing\\tests\\test_unittest_tools.py -> build\\lib.win-amd64-cpython-311\\traits\\testing\\tests\n",
      "      copying traits\\testing\\tests\\__init__.py -> build\\lib.win-amd64-cpython-311\\traits\\testing\\tests\n",
      "      creating build\\lib.win-amd64-cpython-311\\traits\\util\\tests\n",
      "      copying traits\\util\\tests\\test_async_trait_wait.py -> build\\lib.win-amd64-cpython-311\\traits\\util\\tests\n",
      "      copying traits\\util\\tests\\test_camel_case.py -> build\\lib.win-amd64-cpython-311\\traits\\util\\tests\n",
      "      copying traits\\util\\tests\\test_clean_strings.py -> build\\lib.win-amd64-cpython-311\\traits\\util\\tests\n",
      "      copying traits\\util\\tests\\test_deprecated.py -> build\\lib.win-amd64-cpython-311\\traits\\util\\tests\n",
      "      copying traits\\util\\tests\\test_import_symbol.py -> build\\lib.win-amd64-cpython-311\\traits\\util\\tests\n",
      "      copying traits\\util\\tests\\test_message_records.py -> build\\lib.win-amd64-cpython-311\\traits\\util\\tests\n",
      "      copying traits\\util\\tests\\test_record_containers.py -> build\\lib.win-amd64-cpython-311\\traits\\util\\tests\n",
      "      copying traits\\util\\tests\\test_record_events.py -> build\\lib.win-amd64-cpython-311\\traits\\util\\tests\n",
      "      copying traits\\util\\tests\\test_resource.py -> build\\lib.win-amd64-cpython-311\\traits\\util\\tests\n",
      "      copying traits\\util\\tests\\test_traitsui_helpers.py -> build\\lib.win-amd64-cpython-311\\traits\\util\\tests\n",
      "      copying traits\\util\\tests\\test_trait_documenter.py -> build\\lib.win-amd64-cpython-311\\traits\\util\\tests\n",
      "      copying traits\\util\\tests\\test_weakidddict.py -> build\\lib.win-amd64-cpython-311\\traits\\util\\tests\n",
      "      copying traits\\util\\tests\\__init__.py -> build\\lib.win-amd64-cpython-311\\traits\\util\\tests\n",
      "      creating build\\lib.win-amd64-cpython-311\\traits\\examples\\introduction\n",
      "      copying traits\\examples\\introduction\\0_introduction.py -> build\\lib.win-amd64-cpython-311\\traits\\examples\\introduction\n",
      "      copying traits\\examples\\introduction\\1_validation.py -> build\\lib.win-amd64-cpython-311\\traits\\examples\\introduction\n",
      "      copying traits\\examples\\introduction\\2_initialization.py -> build\\lib.win-amd64-cpython-311\\traits\\examples\\introduction\n",
      "      copying traits\\examples\\introduction\\3_observation.py -> build\\lib.win-amd64-cpython-311\\traits\\examples\\introduction\n",
      "      copying traits\\examples\\introduction\\4_properties.py -> build\\lib.win-amd64-cpython-311\\traits\\examples\\introduction\n",
      "      copying traits\\examples\\introduction\\5_documentation.py -> build\\lib.win-amd64-cpython-311\\traits\\examples\\introduction\n",
      "      copying traits\\examples\\introduction\\6_visualization.py -> build\\lib.win-amd64-cpython-311\\traits\\examples\\introduction\n",
      "      copying traits\\examples\\introduction\\default.css -> build\\lib.win-amd64-cpython-311\\traits\\examples\\introduction\n",
      "      copying traits\\examples\\introduction\\index.rst -> build\\lib.win-amd64-cpython-311\\traits\\examples\\introduction\n",
      "      creating build\\lib.win-amd64-cpython-311\\traits\\examples\\introduction\\images\n",
      "      copying traits\\examples\\introduction\\images\\LICENSE.txt -> build\\lib.win-amd64-cpython-311\\traits\\examples\\introduction\\images\n",
      "      copying traits\\examples\\introduction\\images\\sample_0001.png -> build\\lib.win-amd64-cpython-311\\traits\\examples\\introduction\\images\n",
      "      copying traits\\examples\\introduction\\images\\sample_0002.png -> build\\lib.win-amd64-cpython-311\\traits\\examples\\introduction\\images\n",
      "      creating build\\lib.win-amd64-cpython-311\\traits\\tests\\test-data\n",
      "      creating build\\lib.win-amd64-cpython-311\\traits\\tests\\test-data\\historical-pickles\n",
      "      copying traits\\tests\\test-data\\historical-pickles\\README -> build\\lib.win-amd64-cpython-311\\traits\\tests\\test-data\\historical-pickles\n",
      "      copying traits\\tests\\test-data\\historical-pickles\\hipt-t5.2.0-p0-float-ctrait.pkl -> build\\lib.win-amd64-cpython-311\\traits\\tests\\test-data\\historical-pickles\n",
      "      copying traits\\tests\\test-data\\historical-pickles\\hipt-t5.2.0-p1-float-ctrait.pkl -> build\\lib.win-amd64-cpython-311\\traits\\tests\\test-data\\historical-pickles\n",
      "      copying traits\\tests\\test-data\\historical-pickles\\hipt-t5.2.0-p2-float-ctrait.pkl -> build\\lib.win-amd64-cpython-311\\traits\\tests\\test-data\\historical-pickles\n",
      "      copying traits\\tests\\test-data\\historical-pickles\\hipt-t5.2.0-p3-float-ctrait.pkl -> build\\lib.win-amd64-cpython-311\\traits\\tests\\test-data\\historical-pickles\n",
      "      copying traits\\tests\\test-data\\historical-pickles\\hipt-t5.2.0-p4-float-ctrait.pkl -> build\\lib.win-amd64-cpython-311\\traits\\tests\\test-data\\historical-pickles\n",
      "      copying traits\\tests\\test-data\\historical-pickles\\hipt-t5.2.0-p5-float-ctrait.pkl -> build\\lib.win-amd64-cpython-311\\traits\\tests\\test-data\\historical-pickles\n",
      "      copying traits\\tests\\test-data\\historical-pickles\\generate_pickles.py -> build\\lib.win-amd64-cpython-311\\traits\\tests\\test-data\\historical-pickles\n",
      "      running build_ext\n",
      "      building 'traits.ctraits' extension\n",
      "      error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for traits\n",
      "ERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (traits)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Using cached fitz-0.0.1.dev2-py2.py3-none-any.whl.metadata (816 bytes)\n",
      "Collecting configobj (from fitz)\n",
      "  Using cached configobj-5.0.8-py2.py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting configparser (from fitz)\n",
      "  Using cached configparser-7.0.0-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting httplib2 (from fitz)\n",
      "  Using cached httplib2-0.22.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting nibabel (from fitz)\n",
      "  Using cached nibabel-5.2.1-py3-none-any.whl.metadata (8.8 kB)\n",
      "Collecting nipype (from fitz)\n",
      "  Using cached nipype-1.8.6-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: numpy in d:\\chunks_of_different_types\\.venv\\lib\\site-packages (from fitz) (1.26.4)\n",
      "Collecting pandas (from fitz)\n",
      "  Using cached pandas-2.2.2-cp311-cp311-win_amd64.whl.metadata (19 kB)\n",
      "Collecting pyxnat (from fitz)\n",
      "  Using cached pyxnat-1.6.2-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting scipy (from fitz)\n",
      "  Using cached scipy-1.14.0-cp311-cp311-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: six in d:\\chunks_of_different_types\\.venv\\lib\\site-packages (from configobj->fitz) (1.16.0)\n",
      "Collecting pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 (from httplib2->fitz)\n",
      "  Using cached pyparsing-3.1.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: packaging>=17 in d:\\chunks_of_different_types\\.venv\\lib\\site-packages (from nibabel->fitz) (24.1)\n",
      "Requirement already satisfied: click>=6.6.0 in d:\\chunks_of_different_types\\.venv\\lib\\site-packages (from nipype->fitz) (8.1.7)\n",
      "Collecting networkx>=2.0 (from nipype->fitz)\n",
      "  Using cached networkx-3.3-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting prov>=1.5.2 (from nipype->fitz)\n",
      "  Using cached prov-2.0.1-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting pydot>=1.2.3 (from nipype->fitz)\n",
      "  Using cached pydot-2.0.0-py3-none-any.whl.metadata (9.6 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.2 in d:\\chunks_of_different_types\\.venv\\lib\\site-packages (from nipype->fitz) (2.9.0.post0)\n",
      "Collecting rdflib>=5.0.0 (from nipype->fitz)\n",
      "  Using cached rdflib-7.0.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting simplejson>=3.8.0 (from nipype->fitz)\n",
      "  Using cached simplejson-3.19.2-cp311-cp311-win_amd64.whl.metadata (3.2 kB)\n",
      "Collecting traits!=5.0,<6.4,>=4.6 (from nipype->fitz)\n",
      "  Using cached traits-6.3.2.tar.gz (9.5 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting filelock>=3.0.0 (from nipype->fitz)\n",
      "  Using cached filelock-3.15.4-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting etelemetry>=0.2.0 (from nipype->fitz)\n",
      "  Using cached etelemetry-0.3.1-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting looseversion (from nipype->fitz)\n",
      "  Using cached looseversion-1.3.0-py2.py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting pytz>=2020.1 (from pandas->fitz)\n",
      "  Using cached pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->fitz)\n",
      "  Using cached tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting lxml>=4.3 (from pyxnat->fitz)\n",
      "  Using cached lxml-5.2.2-cp311-cp311-win_amd64.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: requests>=2.20 in d:\\chunks_of_different_types\\.venv\\lib\\site-packages (from pyxnat->fitz) (2.32.3)\n",
      "Collecting pathlib>=1.0 (from pyxnat->fitz)\n",
      "  Using cached pathlib-1.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: colorama in d:\\chunks_of_different_types\\.venv\\lib\\site-packages (from click>=6.6.0->nipype->fitz) (0.4.6)\n",
      "Collecting ci-info>=0.2 (from etelemetry>=0.2.0->nipype->fitz)\n",
      "  Using cached ci_info-0.3.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting rdflib>=5.0.0 (from nipype->fitz)\n",
      "  Using cached rdflib-6.3.2-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting isodate<0.7.0,>=0.6.0 (from rdflib>=5.0.0->nipype->fitz)\n",
      "  Using cached isodate-0.6.1-py2.py3-none-any.whl.metadata (9.6 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\chunks_of_different_types\\.venv\\lib\\site-packages (from requests>=2.20->pyxnat->fitz) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\chunks_of_different_types\\.venv\\lib\\site-packages (from requests>=2.20->pyxnat->fitz) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\chunks_of_different_types\\.venv\\lib\\site-packages (from requests>=2.20->pyxnat->fitz) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\chunks_of_different_types\\.venv\\lib\\site-packages (from requests>=2.20->pyxnat->fitz) (2024.6.2)\n",
      "Using cached fitz-0.0.1.dev2-py2.py3-none-any.whl (20 kB)\n",
      "Using cached configobj-5.0.8-py2.py3-none-any.whl (36 kB)\n",
      "Using cached configparser-7.0.0-py3-none-any.whl (16 kB)\n",
      "Using cached httplib2-0.22.0-py3-none-any.whl (96 kB)\n",
      "Using cached nibabel-5.2.1-py3-none-any.whl (3.3 MB)\n",
      "Using cached nipype-1.8.6-py3-none-any.whl (3.2 MB)\n",
      "Using cached scipy-1.14.0-cp311-cp311-win_amd64.whl (44.7 MB)\n",
      "Using cached pandas-2.2.2-cp311-cp311-win_amd64.whl (11.6 MB)\n",
      "Using cached pyxnat-1.6.2-py3-none-any.whl (95 kB)\n",
      "Using cached etelemetry-0.3.1-py3-none-any.whl (6.4 kB)\n",
      "Using cached filelock-3.15.4-py3-none-any.whl (16 kB)\n",
      "Using cached lxml-5.2.2-cp311-cp311-win_amd64.whl (3.8 MB)\n",
      "Using cached networkx-3.3-py3-none-any.whl (1.7 MB)\n",
      "Using cached pathlib-1.0.1-py3-none-any.whl (14 kB)\n",
      "Using cached prov-2.0.1-py3-none-any.whl (421 kB)\n",
      "Using cached pydot-2.0.0-py3-none-any.whl (22 kB)\n",
      "Using cached pyparsing-3.1.2-py3-none-any.whl (103 kB)\n",
      "Using cached pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "Using cached rdflib-6.3.2-py3-none-any.whl (528 kB)\n",
      "Using cached simplejson-3.19.2-cp311-cp311-win_amd64.whl (75 kB)\n",
      "Using cached tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "Using cached looseversion-1.3.0-py2.py3-none-any.whl (8.2 kB)\n",
      "Using cached ci_info-0.3.0-py3-none-any.whl (7.8 kB)\n",
      "Using cached isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
      "Building wheels for collected packages: traits\n",
      "  Building wheel for traits (pyproject.toml): started\n",
      "  Building wheel for traits (pyproject.toml): finished with status 'error'\n",
      "Failed to build traits\n",
      "Requirement already satisfied: tqdm in d:\\chunks_of_different_types\\.venv\\lib\\site-packages (4.66.4)\n",
      "Requirement already satisfied: colorama in d:\\chunks_of_different_types\\.venv\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: bs4 in d:\\chunks_of_different_types\\.venv\\lib\\site-packages (0.0.2)\n",
      "Requirement already satisfied: beautifulsoup4 in d:\\chunks_of_different_types\\.venv\\lib\\site-packages (from bs4) (4.12.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in d:\\chunks_of_different_types\\.venv\\lib\\site-packages (from beautifulsoup4->bs4) (2.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: PyPDF2 in d:\\chunks_of_different_types\\.venv\\lib\\site-packages (3.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: langchain in d:\\chunks_of_different_types\\.venv\\lib\\site-packages (0.2.6)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: PyYAML>=5.3 in d:\\chunks_of_different_types\\.venv\\lib\\site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in d:\\chunks_of_different_types\\.venv\\lib\\site-packages (from langchain) (2.0.31)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in d:\\chunks_of_different_types\\.venv\\lib\\site-packages (from langchain) (3.9.5)\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.10 in d:\\chunks_of_different_types\\.venv\\lib\\site-packages (from langchain) (0.2.10)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in d:\\chunks_of_different_types\\.venv\\lib\\site-packages (from langchain) (0.2.2)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in d:\\chunks_of_different_types\\.venv\\lib\\site-packages (from langchain) (0.1.83)\n",
      "Requirement already satisfied: numpy<2,>=1 in d:\\chunks_of_different_types\\.venv\\lib\\site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in d:\\chunks_of_different_types\\.venv\\lib\\site-packages (from langchain) (2.8.0)\n",
      "Requirement already satisfied: requests<3,>=2 in d:\\chunks_of_different_types\\.venv\\lib\\site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in d:\\chunks_of_different_types\\.venv\\lib\\site-packages (from langchain) (8.4.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in d:\\chunks_of_different_types\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\chunks_of_different_types\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\chunks_of_different_types\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\chunks_of_different_types\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in d:\\chunks_of_different_types\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in d:\\chunks_of_different_types\\.venv\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.10->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in d:\\chunks_of_different_types\\.venv\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.10->langchain) (24.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in d:\\chunks_of_different_types\\.venv\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.5)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in d:\\chunks_of_different_types\\.venv\\lib\\site-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.0 in d:\\chunks_of_different_types\\.venv\\lib\\site-packages (from pydantic<3,>=1->langchain) (2.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in d:\\chunks_of_different_types\\.venv\\lib\\site-packages (from pydantic<3,>=1->langchain) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\chunks_of_different_types\\.venv\\lib\\site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\chunks_of_different_types\\.venv\\lib\\site-packages (from requests<3,>=2->langchain) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\chunks_of_different_types\\.venv\\lib\\site-packages (from requests<3,>=2->langchain) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\chunks_of_different_types\\.venv\\lib\\site-packages (from requests<3,>=2->langchain) (2024.6.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in d:\\chunks_of_different_types\\.venv\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in d:\\chunks_of_different_types\\.venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.10->langchain) (3.0.0)\n",
      "Requirement already satisfied: nltk in d:\\chunks_of_different_types\\.venv\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in d:\\chunks_of_different_types\\.venv\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in d:\\chunks_of_different_types\\.venv\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in d:\\chunks_of_different_types\\.venv\\lib\\site-packages (from nltk) (2024.5.15)\n",
      "Requirement already satisfied: tqdm in d:\\chunks_of_different_types\\.venv\\lib\\site-packages (from nltk) (4.66.4)\n",
      "Requirement already satisfied: colorama in d:\\chunks_of_different_types\\.venv\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install PyMuPDF \n",
    "%pip install fitz\n",
    "%pip install tqdm\n",
    "%pip install bs4\n",
    "%pip install PyPDF2\n",
    "%pip install langchain\n",
    "%pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Ignore all warnings (not recommended unless you know the implications)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Filter out specific warnings\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.icar-crida.res.in/./assets_c/img/Annualreports/AR22.pdf', 'https://www.icar-crida.res.in/./assets_c/img/Annualreports/AR21.pdf', 'https://www.icar-crida.res.in/./assets_c/img/Annualreports/AR20.pdf', 'https://www.icar-crida.res.in/./assets/img/Annualreports/AR19.pdf', 'https://www.icar-crida.res.in/./assets_c/img//Annualreports/AR18-19.pdf', 'https://www.icar-crida.res.in/./assets_c/img//Annualreports/AR17-18.pdf', 'https://www.icar-crida.res.in/assets_c/img/Annualreports/AR16-17.pdf', 'https://www.icar-crida.res.in/assets_c/img/Annualreports/AR15-16.pdf', 'https://www.icar-crida.res.in/assets_c/img/Annualreports/AR14-15.pdf', 'https://www.icar-crida.res.in/assets_c/img/Annualreports/AR13-14.pdf', 'https://www.icar-crida.res.in/assets_c/img/Annualreports/AR12-13.pdf', 'https://www.icar-crida.res.in/assets_c/img/Annualreports/AR11-12.pdf', 'https://www.icar-crida.res.in/assets_c/img/Annualreports/AR10-11.pdf', 'https://www.icar-crida.res.in/assets_c/img/Annualreports/AR09-10.pdf', 'https://www.icar-crida.res.in/assets_c/img/Annualreports/AR08-09.pdf', 'https://www.icar-crida.res.in/assets_c/img/Annualreports/AR07-08.pdf', 'https://www.icar-crida.res.in/assets_c/img/Annualreports/AR06-07.pdf', 'https://www.icar-crida.res.in/assets_c/img/Annualreports/AR04-05.pdf', 'https://www.icar-crida.res.in/assets_c/img/Annualreports/AR03-04.pdf', 'https://www.icar-crida.res.in/assets_c/img/Annualreports/AICRPDA/AR18-19.pdf', 'https://www.icar-crida.res.in/assets_c/img/Annualreports/AICRPDA/AR17-18.pdf', 'https://www.icar-crida.res.in/assets_c/img/Annualreports/AICRPDA/AR16-17.pdf', 'https://www.icar-crida.res.in/assets_c/img/Annualreports/AICRPDA/AR15-16.pdf', 'https://www.icar-crida.res.in/assets_c/img/Annualreports/AICRPAM/AR%202016-17.pdf', 'https://www.icar-crida.res.in/assets_c/img/Annualreports/AICRPAM/2015-16.pdf', 'https://www.icar-crida.res.in/assets_c/img/Annualreports/AICRPAM/2014-15.pdf', 'https://www.icar-crida.res.in/assets_c/img/Annualreports/AICRPAM/2013-14.pdf', 'https://www.icar-crida.res.in/assets_c/img/Annualreports/AICRPAM/2012-13.pdf', 'https://www.icar-crida.res.in/assets_c/img/Annualreports/AICRPAM/2011.pdf', 'https://www.icar-crida.res.in/assets_c/img/Annualreports/NPCC/FR-2004-07.pdf', 'https://www.icar-crida.res.in/assets_c/img/Annualreports/NAIP/NAIP%20Completion%20Report%20Final.pdf', 'https://www.icar-crida.res.in/assets_c/img/Annualreports/NATP/PSR%2099-04.pdf', 'https://www.icar-crida.res.in/assets_c/img/Annualreports/NICRA/TDC/2015-16.pdf', 'https://www.icar-crida.res.in/assets_c/img/Annualreports/NICRA/TDC/2014-15.pdf']\n",
      "Skipping AR22.pdf. Already downloaded.\n",
      "Skipping AR21.pdf. Already downloaded.\n",
      "Skipping AR20.pdf. Already downloaded.\n",
      "Skipping AR19.pdf. Already downloaded.\n",
      "Skipping AR18-19.pdf. Already downloaded.\n",
      "Skipping AR17-18.pdf. Already downloaded.\n",
      "Skipping AR16-17.pdf. Already downloaded.\n",
      "Skipping AR15-16.pdf. Already downloaded.\n",
      "Skipping AR14-15.pdf. Already downloaded.\n",
      "Skipping AR13-14.pdf. Already downloaded.\n",
      "Downloaded AR12-13.pdf to downloaded_pdfs\n",
      "Downloaded AR11-12.pdf to downloaded_pdfs\n",
      "Downloaded AR10-11.pdf to downloaded_pdfs\n",
      "Downloaded AR09-10.pdf to downloaded_pdfs\n",
      "Downloaded AR08-09.pdf to downloaded_pdfs\n",
      "Downloaded AR07-08.pdf to downloaded_pdfs\n",
      "Downloaded AR06-07.pdf to downloaded_pdfs\n",
      "Downloaded AR04-05.pdf to downloaded_pdfs\n",
      "Downloaded AR03-04.pdf to downloaded_pdfs\n",
      "Skipping AR18-19.pdf. Already downloaded.\n",
      "Skipping AR17-18.pdf. Already downloaded.\n",
      "Skipping AR16-17.pdf. Already downloaded.\n",
      "Skipping AR15-16.pdf. Already downloaded.\n",
      "Skipping AR%202016-17.pdf. Already downloaded.\n",
      "Skipping 2015-16.pdf. Already downloaded.\n",
      "Skipping 2014-15.pdf. Already downloaded.\n",
      "Skipping 2013-14.pdf. Already downloaded.\n",
      "Skipping 2012-13.pdf. Already downloaded.\n",
      "Skipping 2011.pdf. Already downloaded.\n",
      "Skipping FR-2004-07.pdf. Already downloaded.\n",
      "Skipping NAIP%20Completion%20Report%20Final.pdf. Already downloaded.\n",
      "Skipping PSR%2099-04.pdf. Already downloaded.\n",
      "Skipping 2015-16.pdf. Already downloaded.\n",
      "Skipping 2014-15.pdf. Already downloaded.\n",
      "PDF download and URL scraping completed!\n",
      "Extracted, cleaned, and saved text from downloaded_pdfs\\AR22.pdf to extracted_texts\\AR22.txt\n",
      "Extracted, cleaned, and saved text from downloaded_pdfs\\AR21.pdf to extracted_texts\\AR21.txt\n",
      "Extracted, cleaned, and saved text from downloaded_pdfs\\AR20.pdf to extracted_texts\\AR20.txt\n",
      "Extracted, cleaned, and saved text from downloaded_pdfs\\AR19.pdf to extracted_texts\\AR19.txt\n",
      "Extracted, cleaned, and saved text from downloaded_pdfs\\AR18-19.pdf to extracted_texts\\AR18-19.txt\n",
      "Extracted, cleaned, and saved text from downloaded_pdfs\\AR17-18.pdf to extracted_texts\\AR17-18.txt\n",
      "Extracted, cleaned, and saved text from downloaded_pdfs\\AR16-17.pdf to extracted_texts\\AR16-17.txt\n",
      "Extracted, cleaned, and saved text from downloaded_pdfs\\AR15-16.pdf to extracted_texts\\AR15-16.txt\n",
      "Extracted, cleaned, and saved text from downloaded_pdfs\\AR14-15.pdf to extracted_texts\\AR14-15.txt\n",
      "Extracted, cleaned, and saved text from downloaded_pdfs\\AR13-14.pdf to extracted_texts\\AR13-14.txt\n",
      "Extracted, cleaned, and saved text from downloaded_pdfs\\AR12-13.pdf to extracted_texts\\AR12-13.txt\n",
      "Extracted, cleaned, and saved text from downloaded_pdfs\\AR11-12.pdf to extracted_texts\\AR11-12.txt\n",
      "Extracted, cleaned, and saved text from downloaded_pdfs\\AR10-11.pdf to extracted_texts\\AR10-11.txt\n",
      "Extracted, cleaned, and saved text from downloaded_pdfs\\AR09-10.pdf to extracted_texts\\AR09-10.txt\n",
      "Extracted, cleaned, and saved text from downloaded_pdfs\\AR08-09.pdf to extracted_texts\\AR08-09.txt\n",
      "Extracted, cleaned, and saved text from downloaded_pdfs\\AR07-08.pdf to extracted_texts\\AR07-08.txt\n",
      "Extracted, cleaned, and saved text from downloaded_pdfs\\AR06-07.pdf to extracted_texts\\AR06-07.txt\n",
      "Extracted, cleaned, and saved text from downloaded_pdfs\\AR04-05.pdf to extracted_texts\\AR04-05.txt\n",
      "Extracted, cleaned, and saved text from downloaded_pdfs\\AR03-04.pdf to extracted_texts\\AR03-04.txt\n",
      "Extracted, cleaned, and saved text from downloaded_pdfs\\AR18-19.pdf to extracted_texts\\AR18-19.txt\n",
      "Extracted, cleaned, and saved text from downloaded_pdfs\\AR17-18.pdf to extracted_texts\\AR17-18.txt\n",
      "Extracted, cleaned, and saved text from downloaded_pdfs\\AR16-17.pdf to extracted_texts\\AR16-17.txt\n",
      "Extracted, cleaned, and saved text from downloaded_pdfs\\AR15-16.pdf to extracted_texts\\AR15-16.txt\n",
      "Extracted, cleaned, and saved text from downloaded_pdfs\\AR%202016-17.pdf to extracted_texts\\AR%202016-17.txt\n",
      "Extracted, cleaned, and saved text from downloaded_pdfs\\2015-16.pdf to extracted_texts\\2015-16.txt\n",
      "Extracted, cleaned, and saved text from downloaded_pdfs\\2014-15.pdf to extracted_texts\\2014-15.txt\n",
      "Extracted, cleaned, and saved text from downloaded_pdfs\\2013-14.pdf to extracted_texts\\2013-14.txt\n",
      "Extracted, cleaned, and saved text from downloaded_pdfs\\2012-13.pdf to extracted_texts\\2012-13.txt\n",
      "Extracted, cleaned, and saved text from downloaded_pdfs\\2011.pdf to extracted_texts\\2011.txt\n",
      "Extracted, cleaned, and saved text from downloaded_pdfs\\FR-2004-07.pdf to extracted_texts\\FR-2004-07.txt\n",
      "Extracted, cleaned, and saved text from downloaded_pdfs\\NAIP%20Completion%20Report%20Final.pdf to extracted_texts\\NAIP%20Completion%20Report%20Final.txt\n",
      "Extracted, cleaned, and saved text from downloaded_pdfs\\PSR%2099-04.pdf to extracted_texts\\PSR%2099-04.txt\n",
      "Extracted, cleaned, and saved text from downloaded_pdfs\\2015-16.pdf to extracted_texts\\2015-16.txt\n",
      "Extracted, cleaned, and saved text from downloaded_pdfs\\2014-15.pdf to extracted_texts\\2014-15.txt\n",
      "Text extraction, cleaning, splitting, and saving completed!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import os\n",
    "import PyPDF2\n",
    "import re\n",
    "\n",
    "# URL of the webpage to scrape\n",
    "url = \"https://www.icar-crida.res.in/publications_annualreports.html\"\n",
    "\n",
    "def fetch_pdf_urls(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        content = BeautifulSoup(response.content, 'html.parser')\n",
    "        links = content.find_all('a')\n",
    "        pdf_links = []\n",
    "        for link in links:\n",
    "            href = link.get('href')\n",
    "            if href and '.pdf' in href:\n",
    "                if not href.startswith('http'):\n",
    "                    href = 'https://www.icar-crida.res.in/' + href.lstrip('/')\n",
    "                pdf_links.append(href)\n",
    "        return pdf_links\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Failed to fetch webpage: {url} with error: {e}\")\n",
    "        return []\n",
    "\n",
    "def download_pdfs(pdf_urls, download_dir='downloaded_pdfs'):\n",
    "    os.makedirs(download_dir, exist_ok=True)\n",
    "    downloaded_files = []\n",
    "    for url in pdf_urls:\n",
    "        filename = url.split('/')[-1]\n",
    "        filepath = os.path.join(download_dir, filename)\n",
    "        if os.path.exists(filepath):\n",
    "            print(f\"Skipping {filename}. Already downloaded.\")\n",
    "            downloaded_files.append(filepath)\n",
    "            continue\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "            with open(filepath, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "            downloaded_files.append(filepath)\n",
    "            print(f\"Downloaded {filename} to {download_dir}\")\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Failed to download {filename}: {e}\")\n",
    "    return downloaded_files\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        text = ''\n",
    "        for page_num in range(len(reader.pages)):\n",
    "            page = reader.pages[page_num]\n",
    "            text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with a single space\n",
    "    return text.strip()\n",
    "\n",
    "# def split_text_into_sections(text, section_length=1000):\n",
    "#     sections = [text[i:i + section_length] for i in range(0, len(text), section_length)]\n",
    "#     return sections\n",
    "\n",
    "# def save_sections_to_file(sections, output_file):\n",
    "#     with open(output_file, 'w', encoding='utf-8') as file:\n",
    "#         for i, section in enumerate(sections):\n",
    "#             file.write(f\"Section {i+1}:\\n\")\n",
    "#             file.write(section + \"\\n\\n\")\n",
    "\n",
    "# Fetch PDF URLs and download PDFs\n",
    "pdf_urls = fetch_pdf_urls(url)\n",
    "\n",
    "# Save PDF URLs to a JSON file\n",
    "with open('icar_crida_report_urls.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(pdf_urls, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(pdf_urls)\n",
    "downloaded_pdfs = download_pdfs(pdf_urls)\n",
    "print(\"PDF download and URL scraping completed!\")\n",
    "\n",
    "# Directory to save the text files\n",
    "text_dir = 'extracted_texts'\n",
    "os.makedirs(text_dir, exist_ok=True)\n",
    "\n",
    "# Process each downloaded PDF\n",
    "for pdf_path in downloaded_pdfs:\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    cleaned_text = clean_text(text)\n",
    "    # sections = split_text_into_sections(cleaned_text)\n",
    "    txt_filename = os.path.basename(pdf_path).replace('.pdf', '.txt')\n",
    "    txt_filepath = os.path.join(text_dir, txt_filename)\n",
    "    # save_sections_to_file(sections, txt_filepath)\n",
    "    print(f\"Extracted, cleaned, and saved text from {pdf_path} to {txt_filepath}\")\n",
    "\n",
    "print(\"Text extraction, cleaning, splitting, and saving completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:   4%|▎         | 1/28 [00:08<03:51,  8.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2011.pdf: 1064 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:   7%|▋         | 2/28 [00:31<07:18, 16.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2012-13.pdf: 706 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  11%|█         | 3/28 [00:41<05:49, 13.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2013-14.pdf: 569 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  14%|█▍        | 4/28 [00:48<04:22, 10.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2014-15.pdf: 774 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  18%|█▊        | 5/28 [00:55<03:43,  9.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2015-16.pdf: 729 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  21%|██▏       | 6/28 [01:05<03:31,  9.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR%202016-17.pdf: 860 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  25%|██▌       | 7/28 [01:12<03:05,  8.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR03-04.pdf: 1133 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  29%|██▊       | 8/28 [01:20<02:54,  8.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR04-05.pdf: 1206 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  32%|███▏      | 9/28 [01:39<03:46, 11.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR06-07.pdf: 1262 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  36%|███▌      | 10/28 [02:07<05:00, 16.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR07-08.pdf: 1331 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  39%|███▉      | 11/28 [02:24<04:49, 17.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR08-09.pdf: 1463 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  43%|████▎     | 12/28 [02:40<04:25, 16.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR09-10.pdf: 1606 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  46%|████▋     | 13/28 [03:04<04:44, 18.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR10-11.pdf: 1632 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  50%|█████     | 14/28 [03:47<06:05, 26.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR11-12.pdf: 1914 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  54%|█████▎    | 15/28 [04:47<07:53, 36.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR12-13.pdf: 1883 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  57%|█████▋    | 16/28 [05:11<06:31, 32.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR13-14.pdf: 1957 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  61%|██████    | 17/28 [05:47<06:09, 33.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR14-15.pdf: 1672 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  64%|██████▍   | 18/28 [06:31<06:08, 36.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR15-16.pdf: 1902 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  68%|██████▊   | 19/28 [07:19<05:59, 39.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR16-17.pdf: 2176 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  71%|███████▏  | 20/28 [07:36<04:24, 33.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR17-18.pdf: 1802 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  75%|███████▌  | 21/28 [08:00<03:33, 30.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR18-19.pdf: 1693 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  79%|███████▊  | 22/28 [08:31<03:04, 30.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR19.pdf: 1766 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  82%|████████▏ | 23/28 [08:53<02:20, 28.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR20.pdf: 1424 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  86%|████████▌ | 24/28 [09:18<01:48, 27.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR21.pdf: 1750 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  89%|████████▉ | 25/28 [09:46<01:22, 27.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR22.pdf: 1633 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  93%|█████████▎| 26/28 [10:04<00:48, 24.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed FR-2004-07.pdf: 1861 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  96%|█████████▋| 27/28 [10:13<00:19, 19.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed NAIP%20Completion%20Report%20Final.pdf: 737 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs: 100%|██████████| 28/28 [10:28<00:00, 22.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed PSR%2099-04.pdf: 1981 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import fitz\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "# Function to extract text from PDF\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        text = \"\"\n",
    "        for pgno in range(len(doc)):\n",
    "            page = doc.load_page(pgno)\n",
    "            text += page.get_text().replace('\\n', ' ')  # Remove \\n characters\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {pdf_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Function to preprocess text and chunk into sentences\n",
    "def preprocess_and_chunk_sentences(text, max_chunk_size=300):\n",
    "    sentences = sent_tokenize(text)  # Tokenize text into sentences\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        tokens = word_tokenize(sentence)  # Tokenize each sentence into words\n",
    "        for token in tokens:\n",
    "            current_chunk.append(token)\n",
    "            if len(' '.join(current_chunk)) >= max_chunk_size:\n",
    "                chunks.append(' '.join(current_chunk))\n",
    "                current_chunk = []\n",
    "\n",
    "    # Append any remaining tokens to the last chunk\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# Function to process PDFs and directly create text files\n",
    "def process_pdfs_and_create_files(pdf_dir='downloaded_pdfs', output_folder='processed_pdfs'):\n",
    "    os.makedirs(output_folder, exist_ok=True)  # Ensure the output folder exists\n",
    "\n",
    "    for filename in tqdm(os.listdir(pdf_dir), desc=\"Processing PDFs\"):\n",
    "        if filename.endswith('.pdf'):\n",
    "            pdf_path = os.path.join(pdf_dir, filename)\n",
    "            text = extract_text_from_pdf(pdf_path)\n",
    "            if text:\n",
    "                # Preprocess and chunk text into sentences\n",
    "                chunks = preprocess_and_chunk_sentences(text)\n",
    "                # Create text file for each PDF\n",
    "                output_file_path = os.path.join(output_folder, f\"{os.path.splitext(filename)[0]}.txt\")\n",
    "                with open(output_file_path, 'w', encoding='utf-8') as f:\n",
    "                    for chunk in chunks:\n",
    "                        f.write(chunk + '\\n\\n')  # Add new lines between chunks\n",
    "                print(f\"Processed {filename}: {len(chunks)} chunks\")\n",
    "\n",
    "# Process PDFs and directly create text files\n",
    "process_pdfs_and_create_files()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs: 100%|██████████| 28/28 [01:53<00:00,  4.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: 2011.pdf, Total Chunks: 1064\n",
      "File: 2012-13.pdf, Total Chunks: 706\n",
      "File: 2013-14.pdf, Total Chunks: 569\n",
      "File: 2014-15.pdf, Total Chunks: 774\n",
      "File: 2015-16.pdf, Total Chunks: 729\n",
      "File: AR%202016-17.pdf, Total Chunks: 860\n",
      "File: AR03-04.pdf, Total Chunks: 1133\n",
      "File: AR04-05.pdf, Total Chunks: 1206\n",
      "File: AR06-07.pdf, Total Chunks: 1262\n",
      "File: AR07-08.pdf, Total Chunks: 1331\n",
      "File: AR08-09.pdf, Total Chunks: 1463\n",
      "File: AR09-10.pdf, Total Chunks: 1606\n",
      "File: AR10-11.pdf, Total Chunks: 1632\n",
      "File: AR11-12.pdf, Total Chunks: 1914\n",
      "File: AR12-13.pdf, Total Chunks: 1883\n",
      "File: AR13-14.pdf, Total Chunks: 1957\n",
      "File: AR14-15.pdf, Total Chunks: 1672\n",
      "File: AR15-16.pdf, Total Chunks: 1902\n",
      "File: AR16-17.pdf, Total Chunks: 2176\n",
      "File: AR17-18.pdf, Total Chunks: 1802\n",
      "File: AR18-19.pdf, Total Chunks: 1693\n",
      "File: AR19.pdf, Total Chunks: 1766\n",
      "File: AR20.pdf, Total Chunks: 1424\n",
      "File: AR21.pdf, Total Chunks: 1750\n",
      "File: AR22.pdf, Total Chunks: 1633\n",
      "File: FR-2004-07.pdf, Total Chunks: 1861\n",
      "File: NAIP%20Completion%20Report%20Final.pdf, Total Chunks: 737\n",
      "File: PSR%2099-04.pdf, Total Chunks: 1981\n",
      "PDF processing completed. Results saved to icar_crida_reports_processed_pdfs.json\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fixed Size Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:   4%|▎         | 1/28 [00:01<00:51,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2011.pdf: 1090 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:   7%|▋         | 2/28 [00:04<01:02,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2012-13.pdf: 723 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  11%|█         | 3/28 [00:06<00:48,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2013-14.pdf: 583 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  14%|█▍        | 4/28 [00:07<00:42,  1.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2014-15.pdf: 793 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  18%|█▊        | 5/28 [00:08<00:37,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2015-16.pdf: 746 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  21%|██▏       | 6/28 [00:11<00:39,  1.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR%202016-17.pdf: 880 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  25%|██▌       | 7/28 [00:13<00:39,  1.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR03-04.pdf: 1162 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  29%|██▊       | 8/28 [00:16<00:48,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR04-05.pdf: 1237 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  32%|███▏      | 9/28 [00:18<00:44,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR06-07.pdf: 1295 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  36%|███▌      | 10/28 [00:22<00:47,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR07-08.pdf: 1367 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  39%|███▉      | 11/28 [00:25<00:46,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR08-09.pdf: 1501 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  43%|████▎     | 12/28 [00:27<00:42,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR09-10.pdf: 1649 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  46%|████▋     | 13/28 [00:31<00:44,  2.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR10-11.pdf: 1676 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  50%|█████     | 14/28 [00:34<00:44,  3.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR11-12.pdf: 1966 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  54%|█████▎    | 15/28 [00:38<00:44,  3.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR12-13.pdf: 1934 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  57%|█████▋    | 16/28 [00:42<00:41,  3.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR13-14.pdf: 2009 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  61%|██████    | 17/28 [00:47<00:42,  3.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR14-15.pdf: 1716 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  64%|██████▍   | 18/28 [00:52<00:42,  4.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR15-16.pdf: 1950 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  68%|██████▊   | 19/28 [00:59<00:46,  5.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR16-17.pdf: 2231 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  71%|███████▏  | 20/28 [01:02<00:36,  4.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR17-18.pdf: 1849 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  75%|███████▌  | 21/28 [01:06<00:30,  4.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR18-19.pdf: 1737 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  79%|███████▊  | 22/28 [01:10<00:25,  4.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR19.pdf: 1810 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  82%|████████▏ | 23/28 [01:14<00:20,  4.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR20.pdf: 1459 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  86%|████████▌ | 24/28 [01:19<00:17,  4.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR21.pdf: 1794 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  89%|████████▉ | 25/28 [01:23<00:12,  4.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR22.pdf: 1675 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  93%|█████████▎| 26/28 [01:26<00:07,  3.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed FR-2004-07.pdf: 1907 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  96%|█████████▋| 27/28 [01:28<00:03,  3.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed NAIP%20Completion%20Report%20Final.pdf: 756 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs: 100%|██████████| 28/28 [01:32<00:00,  3.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed PSR%2099-04.pdf: 2030 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Character-based chunking --- 1m 32.8s\n",
    "import fitz\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "# Function to extract text from PDF\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        text = \"\"\n",
    "        for pgno in range(len(doc)):\n",
    "            page = doc.load_page(pgno)\n",
    "            text += page.get_text().replace('\\n', ' ')  # Remove \\n characters\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {pdf_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Function to preprocess text and chunk into character-based chunks after NLTK tokenization\n",
    "def preprocess_and_chunk_characters(text, max_chunk_size=300):\n",
    "    sentences = sent_tokenize(text)  # Tokenize text into sentences using NLTK\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "\n",
    "    for sentence in sentences:\n",
    "        tokens = word_tokenize(sentence)  # Tokenize each sentence into words using NLTK\n",
    "        for token in tokens:\n",
    "            if len(current_chunk) + len(token) >= max_chunk_size:\n",
    "                chunks.append(current_chunk)\n",
    "                current_chunk = \"\"\n",
    "            current_chunk += token + \" \"\n",
    "\n",
    "    # Append any remaining characters to the last chunk\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# Function to process PDFs and directly create text files\n",
    "def process_pdfs_and_create_files(pdf_dir='downloaded_pdfs', output_folder='character_chunks'):\n",
    "    os.makedirs(output_folder, exist_ok=True)  # Ensure the output folder exists\n",
    "\n",
    "    for filename in tqdm(os.listdir(pdf_dir), desc=\"Processing PDFs\"):\n",
    "        if filename.endswith('.pdf'):\n",
    "            pdf_path = os.path.join(pdf_dir, filename)\n",
    "            text = extract_text_from_pdf(pdf_path)\n",
    "            if text:\n",
    "                # Preprocess and chunk text into characters\n",
    "                chunks = preprocess_and_chunk_characters(text)\n",
    "                # Create text file for each PDF\n",
    "                output_file_path = os.path.join(output_folder, f\"{os.path.splitext(filename)[0]}.txt\")\n",
    "                with open(output_file_path, 'w', encoding='utf-8') as f:\n",
    "                    for chunk in chunks:\n",
    "                        f.write(chunk + '\\n\\n')  # Add new lines between chunks\n",
    "                print(f\"Processed {filename}: {len(chunks)} chunks\")\n",
    "\n",
    "# Process PDFs and directly create text files\n",
    "process_pdfs_and_create_files()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:   4%|▎         | 1/28 [00:02<01:02,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2011.pdf: 601 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:   7%|▋         | 2/28 [00:04<01:00,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2012-13.pdf: 397 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  11%|█         | 3/28 [00:06<00:47,  1.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2013-14.pdf: 338 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  14%|█▍        | 4/28 [00:07<00:43,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2014-15.pdf: 440 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  18%|█▊        | 5/28 [00:09<00:41,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2015-16.pdf: 420 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  21%|██▏       | 6/28 [00:11<00:43,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR%202016-17.pdf: 500 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  25%|██▌       | 7/28 [00:14<00:48,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR03-04.pdf: 623 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  29%|██▊       | 8/28 [00:19<01:01,  3.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR04-05.pdf: 653 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  32%|███▏      | 9/28 [00:21<00:50,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR06-07.pdf: 688 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  36%|███▌      | 10/28 [00:23<00:47,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR07-08.pdf: 726 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  39%|███▉      | 11/28 [00:26<00:45,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR08-09.pdf: 789 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  43%|████▎     | 12/28 [00:28<00:39,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR09-10.pdf: 871 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  46%|████▋     | 13/28 [00:31<00:40,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR10-11.pdf: 884 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  50%|█████     | 14/28 [00:36<00:47,  3.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR11-12.pdf: 1029 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  54%|█████▎    | 15/28 [00:40<00:46,  3.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR12-13.pdf: 1013 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  57%|█████▋    | 16/28 [00:43<00:39,  3.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR13-14.pdf: 1040 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  61%|██████    | 17/28 [00:49<00:44,  4.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR14-15.pdf: 889 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  64%|██████▍   | 18/28 [00:54<00:44,  4.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR15-16.pdf: 1023 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  68%|██████▊   | 19/28 [01:03<00:52,  5.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR16-17.pdf: 1163 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  71%|███████▏  | 20/28 [01:08<00:43,  5.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR17-18.pdf: 968 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  75%|███████▌  | 21/28 [01:12<00:36,  5.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR18-19.pdf: 910 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  79%|███████▊  | 22/28 [01:16<00:27,  4.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR19.pdf: 952 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  82%|████████▏ | 23/28 [01:20<00:22,  4.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR20.pdf: 797 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  86%|████████▌ | 24/28 [01:25<00:18,  4.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR21.pdf: 972 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  89%|████████▉ | 25/28 [01:29<00:13,  4.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR22.pdf: 871 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  93%|█████████▎| 26/28 [01:32<00:08,  4.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed FR-2004-07.pdf: 1012 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  96%|█████████▋| 27/28 [01:34<00:03,  3.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed NAIP%20Completion%20Report%20Final.pdf: 393 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs: 100%|██████████| 28/28 [01:38<00:00,  3.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed PSR%2099-04.pdf: 1053 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Word-based chunking --- 1m 38.7s\n",
    "import fitz\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "# Function to extract text from PDF\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        text = \"\"\n",
    "        for pgno in range(len(doc)):\n",
    "            page = doc.load_page(pgno)\n",
    "            text += page.get_text().replace('\\n', ' ')  # Remove \\n characters\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {pdf_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Function to preprocess text and chunk into word-based chunks\n",
    "def word_based_chunking(text, max_words_per_chunk=100):\n",
    "    words = word_tokenize(text)  # Tokenize text into words using NLTK\n",
    "    chunks = [' '.join(words[i:i + max_words_per_chunk]) for i in range(0, len(words), max_words_per_chunk)]\n",
    "    return chunks\n",
    "\n",
    "\n",
    "\n",
    "# Function to process PDFs and directly create text files\n",
    "def process_pdfs_and_create_files(pdf_dir='downloaded_pdfs', output_folder='word_chunks'):\n",
    "    os.makedirs(output_folder, exist_ok=True)  # Ensure the output folder exists\n",
    "\n",
    "    for filename in tqdm(os.listdir(pdf_dir), desc=\"Processing PDFs\"):\n",
    "        if filename.endswith('.pdf'):\n",
    "            pdf_path = os.path.join(pdf_dir, filename)\n",
    "            text = extract_text_from_pdf(pdf_path)\n",
    "            if text:\n",
    "                # Preprocess and chunk text into words\n",
    "                chunks = word_based_chunking(text)\n",
    "                # Create text file for each PDF\n",
    "                output_file_path = os.path.join(output_folder, f\"{os.path.splitext(filename)[0]}.txt\")\n",
    "                with open(output_file_path, 'w', encoding='utf-8') as f:\n",
    "                    for chunk in chunks:\n",
    "                        f.write(chunk + '\\n\\n')  # Add new lines between chunks\n",
    "                print(f\"Processed {filename}: {len(chunks)} chunks\")\n",
    "\n",
    "# Process PDFs and directly create text files\n",
    "process_pdfs_and_create_files()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:   4%|▎         | 1/28 [00:01<00:31,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2011.pdf: 514 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:   7%|▋         | 2/28 [00:02<00:34,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2012-13.pdf: 387 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  11%|█         | 3/28 [00:03<00:28,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2013-14.pdf: 309 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  14%|█▍        | 4/28 [00:04<00:24,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2014-15.pdf: 322 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  18%|█▊        | 5/28 [00:05<00:26,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2015-16.pdf: 382 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  21%|██▏       | 6/28 [00:08<00:38,  1.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR%202016-17.pdf: 374 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  25%|██▌       | 7/28 [00:10<00:35,  1.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR03-04.pdf: 498 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  29%|██▊       | 8/28 [00:12<00:39,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR04-05.pdf: 520 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  32%|███▏      | 9/28 [00:14<00:33,  1.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR06-07.pdf: 510 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  36%|███▌      | 10/28 [00:15<00:30,  1.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR07-08.pdf: 541 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  39%|███▉      | 11/28 [00:17<00:28,  1.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR08-09.pdf: 616 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  43%|████▎     | 12/28 [00:18<00:26,  1.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR09-10.pdf: 704 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  46%|████▋     | 13/28 [00:22<00:33,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR10-11.pdf: 660 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  50%|█████     | 14/28 [00:25<00:33,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR11-12.pdf: 708 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  54%|█████▎    | 15/28 [00:28<00:35,  2.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR12-13.pdf: 743 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  57%|█████▋    | 16/28 [00:30<00:29,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR13-14.pdf: 817 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  61%|██████    | 17/28 [00:35<00:33,  3.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR14-15.pdf: 688 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  64%|██████▍   | 18/28 [00:39<00:33,  3.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR15-16.pdf: 675 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  68%|██████▊   | 19/28 [00:45<00:38,  4.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR16-17.pdf: 884 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  71%|███████▏  | 20/28 [00:47<00:28,  3.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR17-18.pdf: 909 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  75%|███████▌  | 21/28 [00:51<00:25,  3.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR18-19.pdf: 694 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  79%|███████▊  | 22/28 [00:54<00:20,  3.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR19.pdf: 681 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  82%|████████▏ | 23/28 [00:58<00:17,  3.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR20.pdf: 588 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  86%|████████▌ | 24/28 [01:02<00:15,  3.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR21.pdf: 710 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  89%|████████▉ | 25/28 [01:06<00:11,  3.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR22.pdf: 643 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  93%|█████████▎| 26/28 [01:08<00:06,  3.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed FR-2004-07.pdf: 738 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  96%|█████████▋| 27/28 [01:09<00:02,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed NAIP%20Completion%20Report%20Final.pdf: 511 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs: 100%|██████████| 28/28 [01:12<00:00,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed PSR%2099-04.pdf: 955 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Sentence-based chunking --- 1m 12.3s\n",
    "import fitz\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "# Function to extract text from PDF\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        text = \"\"\n",
    "        for pgno in range(len(doc)):\n",
    "            page = doc.load_page(pgno)\n",
    "            text += page.get_text().replace('\\n', ' ')  # Remove \\n characters\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {pdf_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "# Function to preprocess text and chunk into sentence-based chunks\n",
    "def sentence_based_chunking(text, max_sentences_per_chunk=5):\n",
    "    sentences = sent_tokenize(text)  # Tokenize text into sentences using NLTK\n",
    "    chunks = [' '.join(sentences[i:i + max_sentences_per_chunk]) for i in range(0, len(sentences), max_sentences_per_chunk)]\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# Function to process PDFs and directly create text files\n",
    "def process_pdfs_and_create_files(pdf_dir='downloaded_pdfs', output_folder='sentence_chunks'):\n",
    "    os.makedirs(output_folder, exist_ok=True)  # Ensure the output folder exists\n",
    "\n",
    "    for filename in tqdm(os.listdir(pdf_dir), desc=\"Processing PDFs\"):\n",
    "        if filename.endswith('.pdf'):\n",
    "            pdf_path = os.path.join(pdf_dir, filename)\n",
    "            text = extract_text_from_pdf(pdf_path)\n",
    "            if text:\n",
    "                # Preprocess and chunk text into words\n",
    "                chunks = sentence_based_chunking(text)\n",
    "                # Create text file for each PDF\n",
    "                output_file_path = os.path.join(output_folder, f\"{os.path.splitext(filename)[0]}.txt\")\n",
    "                with open(output_file_path, 'w', encoding='utf-8') as f:\n",
    "                    for chunk in chunks:\n",
    "                        f.write(chunk + '\\n\\n')  # Add new lines between chunks\n",
    "                print(f\"Processed {filename}: {len(chunks)} chunks\")\n",
    "\n",
    "# Process PDFs and directly create text files\n",
    "process_pdfs_and_create_files()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:   4%|▎         | 1/28 [00:01<00:40,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2011.pdf: 2565 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:   7%|▋         | 2/28 [00:02<00:37,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2012-13.pdf: 1929 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  11%|█         | 3/28 [00:04<00:32,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2013-14.pdf: 1539 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  14%|█▍        | 4/28 [00:04<00:27,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2014-15.pdf: 1604 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  18%|█▊        | 5/28 [00:05<00:23,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2015-16.pdf: 1903 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  21%|██▏       | 6/28 [00:07<00:28,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR%202016-17.pdf: 1863 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  25%|██▌       | 7/28 [00:09<00:32,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR03-04.pdf: 2482 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  29%|██▊       | 8/28 [00:12<00:38,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR04-05.pdf: 2595 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  32%|███▏      | 9/28 [00:13<00:33,  1.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR06-07.pdf: 2543 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  36%|███▌      | 10/28 [00:16<00:34,  1.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR07-08.pdf: 2699 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  39%|███▉      | 11/28 [00:18<00:34,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR08-09.pdf: 3076 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  43%|████▎     | 12/28 [00:20<00:32,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR09-10.pdf: 3515 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  46%|████▋     | 13/28 [00:23<00:34,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR10-11.pdf: 3293 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  50%|█████     | 14/28 [00:27<00:39,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR11-12.pdf: 3534 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  54%|█████▎    | 15/28 [00:30<00:40,  3.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR12-13.pdf: 3710 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  57%|█████▋    | 16/28 [00:33<00:36,  3.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR13-14.pdf: 4081 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  61%|██████    | 17/28 [00:37<00:36,  3.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR14-15.pdf: 3434 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  64%|██████▍   | 18/28 [00:42<00:36,  3.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR15-16.pdf: 3370 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  68%|██████▊   | 19/28 [00:48<00:39,  4.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR16-17.pdf: 4412 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  71%|███████▏  | 20/28 [00:50<00:30,  3.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR17-18.pdf: 4538 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  75%|███████▌  | 21/28 [00:54<00:26,  3.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR18-19.pdf: 3466 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  79%|███████▊  | 22/28 [00:57<00:20,  3.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR19.pdf: 3399 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  82%|████████▏ | 23/28 [01:00<00:16,  3.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR20.pdf: 2932 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  86%|████████▌ | 24/28 [01:04<00:13,  3.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR21.pdf: 3544 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  89%|████████▉ | 25/28 [01:07<00:10,  3.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR22.pdf: 3207 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  93%|█████████▎| 26/28 [01:10<00:06,  3.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed FR-2004-07.pdf: 3683 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  96%|█████████▋| 27/28 [01:11<00:02,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed NAIP%20Completion%20Report%20Final.pdf: 2549 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs: 100%|██████████| 28/28 [01:15<00:00,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed PSR%2099-04.pdf: 4769 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Slide Window chunking --- 1m 15.6s\n",
    "import fitz\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "# Function to extract text from PDF\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        text = \"\"\n",
    "        for pgno in range(len(doc)):\n",
    "            page = doc.load_page(pgno)\n",
    "            text += page.get_text().replace('\\n', ' ')  # Remove \\n characters\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {pdf_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "\n",
    "# Function to preprocess text and perform slide window chunking based on sentences\n",
    "def slide_window_chunking(text, window_size=5):\n",
    "    sentences = sent_tokenize(text)  # Tokenize text into sentences using NLTK\n",
    "    chunks = []\n",
    "    for i in range(len(sentences) - window_size + 1):\n",
    "        chunk = ' '.join(sentences[i:i + window_size])\n",
    "        chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# Function to process PDFs and directly create text files\n",
    "def process_pdfs_and_create_files(pdf_dir='downloaded_pdfs', output_folder='window_chunks'):\n",
    "    os.makedirs(output_folder, exist_ok=True)  # Ensure the output folder exists\n",
    "\n",
    "    for filename in tqdm(os.listdir(pdf_dir), desc=\"Processing PDFs\"):\n",
    "        if filename.endswith('.pdf'):\n",
    "            pdf_path = os.path.join(pdf_dir, filename)\n",
    "            text = extract_text_from_pdf(pdf_path)\n",
    "            if text:\n",
    "                # Preprocess and chunk text into words\n",
    "                chunks = slide_window_chunking(text)\n",
    "                # Create text file for each PDF\n",
    "                output_file_path = os.path.join(output_folder, f\"{os.path.splitext(filename)[0]}.txt\")\n",
    "                with open(output_file_path, 'w', encoding='utf-8') as f:\n",
    "                    for chunk in chunks:\n",
    "                        f.write(chunk + '\\n\\n')  # Add new lines between chunks\n",
    "                print(f\"Processed {filename}: {len(chunks)} chunks\")\n",
    "\n",
    "# Process PDFs and directly create text files\n",
    "process_pdfs_and_create_files()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:   0%|          | 0/28 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:   4%|▎         | 1/28 [00:02<00:55,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2011.pdf: 1199 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:   7%|▋         | 2/28 [00:04<01:02,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2012-13.pdf: 791 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  11%|█         | 3/28 [00:05<00:45,  1.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2013-14.pdf: 674 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  14%|█▍        | 4/28 [00:07<00:39,  1.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2014-15.pdf: 877 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  18%|█▊        | 5/28 [00:08<00:34,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2015-16.pdf: 838 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  21%|██▏       | 6/28 [00:10<00:36,  1.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR%202016-17.pdf: 997 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  25%|██▌       | 7/28 [00:13<00:41,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR03-04.pdf: 1243 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  29%|██▊       | 8/28 [00:17<00:53,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR04-05.pdf: 1303 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  32%|███▏      | 9/28 [00:20<00:52,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR06-07.pdf: 1374 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  36%|███▌      | 10/28 [00:23<00:52,  2.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR07-08.pdf: 1450 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  39%|███▉      | 11/28 [00:26<00:50,  2.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR08-09.pdf: 1575 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  43%|████▎     | 12/28 [00:29<00:47,  2.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR09-10.pdf: 1740 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  46%|████▋     | 13/28 [00:34<00:53,  3.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR10-11.pdf: 1766 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  50%|█████     | 14/28 [00:38<00:52,  3.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR11-12.pdf: 2055 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  54%|█████▎    | 15/28 [00:43<00:52,  4.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR12-13.pdf: 2024 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  57%|█████▋    | 16/28 [00:47<00:47,  3.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR13-14.pdf: 2077 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  61%|██████    | 17/28 [00:53<00:51,  4.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR14-15.pdf: 1775 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  64%|██████▍   | 18/28 [00:59<00:50,  5.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR15-16.pdf: 2044 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  68%|██████▊   | 19/28 [01:08<00:55,  6.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR16-17.pdf: 2324 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  71%|███████▏  | 20/28 [01:11<00:42,  5.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR17-18.pdf: 1934 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  75%|███████▌  | 21/28 [01:17<00:37,  5.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR18-19.pdf: 1818 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  79%|███████▊  | 22/28 [01:21<00:30,  5.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR19.pdf: 1901 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  82%|████████▏ | 23/28 [01:25<00:23,  4.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR20.pdf: 1592 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  86%|████████▌ | 24/28 [01:29<00:18,  4.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR21.pdf: 1942 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  89%|████████▉ | 25/28 [01:33<00:13,  4.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR22.pdf: 1739 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  93%|█████████▎| 26/28 [01:36<00:08,  4.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed FR-2004-07.pdf: 2021 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  96%|█████████▋| 27/28 [01:38<00:03,  3.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed NAIP%20Completion%20Report%20Final.pdf: 784 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs: 100%|██████████| 28/28 [01:44<00:00,  3.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed PSR%2099-04.pdf: 2103 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Sliding Window chunking --- 1m 45.0s\n",
    "import fitz\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "# Function to extract text from PDF\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        text = \"\"\n",
    "        for pgno in range(len(doc)):\n",
    "            page = doc.load_page(pgno)\n",
    "            text += page.get_text().replace('\\n', ' ')  # Remove \\n characters\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {pdf_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "\n",
    "# Function to perform slide window chunking based on words\n",
    "def slide_window_chunking(text, window_size=100, overlap=50):\n",
    "    words = word_tokenize(text)  # Tokenize text into words using NLTK\n",
    "    chunks = []\n",
    "    for i in range(0, len(words) - window_size + 1, overlap):\n",
    "        chunk = ' '.join(words[i:i + window_size])\n",
    "        chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# Function to process PDFs and directly create text files\n",
    "def process_pdfs_and_create_files(pdf_dir='downloaded_pdfs', output_folder='sliding_window_chunks'):\n",
    "    os.makedirs(output_folder, exist_ok=True)  # Ensure the output folder exists\n",
    "\n",
    "    for filename in tqdm(os.listdir(pdf_dir), desc=\"Processing PDFs\"):\n",
    "        if filename.endswith('.pdf'):\n",
    "            pdf_path = os.path.join(pdf_dir, filename)\n",
    "            text = extract_text_from_pdf(pdf_path)\n",
    "            if text:\n",
    "                # Preprocess and chunk text into words\n",
    "                chunks = slide_window_chunking(text)\n",
    "                # Create text file for each PDF\n",
    "                output_file_path = os.path.join(output_folder, f\"{os.path.splitext(filename)[0]}.txt\")\n",
    "                with open(output_file_path, 'w', encoding='utf-8') as f:\n",
    "                    for chunk in chunks:\n",
    "                        f.write(chunk + '\\n\\n')  # Add new lines between chunks\n",
    "                print(f\"Processed {filename}: {len(chunks)} chunks\")\n",
    "\n",
    "# Process PDFs and directly create text files\n",
    "process_pdfs_and_create_files()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:   4%|▎         | 1/28 [00:01<00:31,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2011.pdf: 971 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:   7%|▋         | 2/28 [00:03<00:47,  1.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2012-13.pdf: 708 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  11%|█         | 3/28 [00:04<00:35,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2013-14.pdf: 557 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  14%|█▍        | 4/28 [00:05<00:35,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2014-15.pdf: 724 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  18%|█▊        | 5/28 [00:06<00:28,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2015-16.pdf: 689 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  21%|██▏       | 6/28 [00:08<00:30,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR%202016-17.pdf: 751 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  25%|██▌       | 7/28 [00:09<00:28,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR03-04.pdf: 1031 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  29%|██▊       | 8/28 [00:12<00:33,  1.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR04-05.pdf: 1166 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  32%|███▏      | 9/28 [00:13<00:28,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR06-07.pdf: 1165 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  36%|███▌      | 10/28 [00:14<00:27,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR07-08.pdf: 1288 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  39%|███▉      | 11/28 [00:16<00:29,  1.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR08-09.pdf: 1465 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  43%|████▎     | 12/28 [00:18<00:24,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR09-10.pdf: 1658 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  46%|████▋     | 13/28 [00:21<00:29,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR10-11.pdf: 1612 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  50%|█████     | 14/28 [00:23<00:29,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR11-12.pdf: 1813 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  54%|█████▎    | 15/28 [00:26<00:30,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR12-13.pdf: 1703 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  57%|█████▋    | 16/28 [00:28<00:26,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR13-14.pdf: 1827 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  61%|██████    | 17/28 [00:31<00:28,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR14-15.pdf: 1649 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  64%|██████▍   | 18/28 [00:35<00:28,  2.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR15-16.pdf: 1695 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  68%|██████▊   | 19/28 [00:41<00:34,  3.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR16-17.pdf: 2142 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  71%|███████▏  | 20/28 [00:42<00:25,  3.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR17-18.pdf: 1764 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  75%|███████▌  | 21/28 [00:46<00:22,  3.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR18-19.pdf: 1577 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  79%|███████▊  | 22/28 [00:48<00:18,  3.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR19.pdf: 1594 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  82%|████████▏ | 23/28 [00:51<00:14,  2.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR20.pdf: 1341 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  86%|████████▌ | 24/28 [00:54<00:12,  3.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR21.pdf: 1646 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  89%|████████▉ | 25/28 [00:57<00:09,  3.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AR22.pdf: 1529 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  93%|█████████▎| 26/28 [00:59<00:05,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed FR-2004-07.pdf: 1812 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  96%|█████████▋| 27/28 [01:00<00:02,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed NAIP%20Completion%20Report%20Final.pdf: 689 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs: 100%|██████████| 28/28 [01:03<00:00,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed PSR%2099-04.pdf: 1994 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Context based chunking --- 1m 4.0s\n",
    "import fitz\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "# Function to extract text from PDF\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        text = \"\"\n",
    "        for pgno in range(len(doc)):\n",
    "            page = doc.load_page(pgno)\n",
    "            text += page.get_text().replace('\\n', ' ')  # Remove \\n characters\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {pdf_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "\n",
    "# Function to perform context-based chunking based on sentences\n",
    "def context_based_chunking(text, max_words_per_chunk=200):\n",
    "    sentences = sent_tokenize(text)  # Tokenize text into sentences using NLTK\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        words = sentence.split()  # Split sentence into words\n",
    "        if len(' '.join(current_chunk)) + len(words) <= max_words_per_chunk:\n",
    "            current_chunk.extend(words)\n",
    "        else:\n",
    "            if current_chunk:\n",
    "                chunks.append(' '.join(current_chunk))\n",
    "                current_chunk = words\n",
    "\n",
    "    # Append any remaining words to the last chunk\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# Function to process PDFs and directly create text files\n",
    "def process_pdfs_and_create_files(pdf_dir='downloaded_pdfs', output_folder='context_chunks'):\n",
    "    os.makedirs(output_folder, exist_ok=True)  # Ensure the output folder exists\n",
    "\n",
    "    for filename in tqdm(os.listdir(pdf_dir), desc=\"Processing PDFs\"):\n",
    "        if filename.endswith('.pdf'):\n",
    "            pdf_path = os.path.join(pdf_dir, filename)\n",
    "            text = extract_text_from_pdf(pdf_path)\n",
    "            if text:\n",
    "                # Preprocess and chunk text into words\n",
    "                chunks = context_based_chunking(text)\n",
    "                # Create text file for each PDF\n",
    "                output_file_path = os.path.join(output_folder, f\"{os.path.splitext(filename)[0]}.txt\")\n",
    "                with open(output_file_path, 'w', encoding='utf-8') as f:\n",
    "                    for chunk in chunks:\n",
    "                        f.write(chunk + '\\n\\n')  # Add new lines between chunks\n",
    "                print(f\"Processed {filename}: {len(chunks)} chunks\")\n",
    "\n",
    "# Process PDFs and directly create text files\n",
    "process_pdfs_and_create_files()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import fitz\n",
    "# import os\n",
    "# import json\n",
    "# from tqdm import tqdm\n",
    "# from nltk.tokenize import sent_tokenize\n",
    "# from langchain_text_splitters import NLTKTextSplitter\n",
    "# from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "# # Function to extract text from PDF\n",
    "# def extract_text_from_pdf(pdf_path):\n",
    "#     try:\n",
    "#         doc = fitz.open(pdf_path)\n",
    "#         text = \"\"\n",
    "#         for pgno in range(len(doc)):\n",
    "#             page = doc.load_page(pgno)\n",
    "#             text += page.get_text().replace('\\n', ' ')  # Remove \\n characters\n",
    "#         return text\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error reading {pdf_path}: {e}\")\n",
    "#         return \"\"\n",
    "\n",
    "# # Function to preprocess text and chunk into sentences\n",
    "# def preprocess_and_chunk_sentences(text, max_chunk_size=300):\n",
    "#     sentences = sent_tokenize(text)  # Tokenize text into sentences\n",
    "#     chunks = []\n",
    "#     current_chunk = []\n",
    "\n",
    "#     for sentence in sentences:\n",
    "#         tokens = word_tokenize(sentence)  # Tokenize each sentence into words\n",
    "#         for token in tokens:\n",
    "#             current_chunk.append(token)\n",
    "#             if len(' '.join(current_chunk)) >= max_chunk_size:\n",
    "#                 chunks.append(' '.join(current_chunk))\n",
    "#                 current_chunk = []\n",
    "\n",
    "#     # Append any remaining tokens to the last chunk\n",
    "#     if current_chunk:\n",
    "#         chunks.append(' '.join(current_chunk))\n",
    "\n",
    "#     return chunks\n",
    "\n",
    "\n",
    "# # Function to process PDFs and preprocess text\n",
    "# def process_pdfs(pdf_dir='downloaded_pdfs'):\n",
    "#     pdf_texts = []\n",
    "#     for filename in tqdm(os.listdir(pdf_dir), desc=\"Processing PDFs\"):\n",
    "#         if filename.endswith('.pdf'):\n",
    "#             pdf_path = os.path.join(pdf_dir, filename)\n",
    "#             text = extract_text_from_pdf(pdf_path)\n",
    "#             if text:\n",
    "#                 # Preprocess and chunk text into sentences\n",
    "#                 chunks = preprocess_and_chunk_sentences(text)\n",
    "#                 pdf_texts.append({\n",
    "#                     'filename': filename,\n",
    "#                     'chunks': chunks,\n",
    "#                     'total_chunks': len(chunks)\n",
    "#                 })\n",
    "#     return pdf_texts\n",
    "\n",
    "# # Process PDFs and preprocess text\n",
    "# pdf_texts = process_pdfs()\n",
    "\n",
    "# # Print number of chunks for each PDF\n",
    "# for pdf in pdf_texts:\n",
    "#     print(f\"File: {pdf['filename']}, Total Chunks: {pdf['total_chunks']}\")\n",
    "\n",
    "# # Dump pdf_texts to JSON file\n",
    "# output_file = 'icar_crida_reports_chunking_pdfs'\n",
    "# with open(output_file, 'w', encoding='utf-8') as f:\n",
    "#     json.dump(pdf_texts, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# print(f\"PDF processing completed. Results saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of items in 'icar_crida_reports_processed_pdfs.json': 28\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# num_items = get_json_length(output_file)\n",
    "# print(f\"Number of items in '{json_file}': {num_items}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored chunks from 'icar_crida_reports_processed_pdfs.json' in 'icar_crida_reports_chunking_pdfs' as text files\n"
     ]
    }
   ],
   "source": [
    "# import json\n",
    "# import os\n",
    "\n",
    "# # Function to read JSON file and store each item's chunks in separate text files\n",
    "# def store_chunks_in_text_files(json_file, output_folder):\n",
    "#     try:\n",
    "#         # Ensure the output folder exists; create it if it doesn't\n",
    "#         os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "#         with open(json_file, 'r', encoding='utf-8') as f:\n",
    "#             data = json.load(f)\n",
    "\n",
    "#             for item in data:\n",
    "#                 # Extract filename from item (assuming 'filename' is a key in your JSON)\n",
    "#                 filename = item.get('filename')\n",
    "#                 if filename:\n",
    "#                     # Construct file path with .txt extension\n",
    "#                     file_path = os.path.join(output_folder, filename + '.txt')\n",
    "\n",
    "#                     # Extract chunks to write to text file\n",
    "#                     chunks = item.get('chunks', [])\n",
    "\n",
    "#                     # Write chunks to text file\n",
    "#                     with open(file_path, 'w', encoding='utf-8') as out_f:\n",
    "#                         for chunk in chunks:\n",
    "#                             out_f.write(chunk + '\\n\\n')  # Add new lines between chunks\n",
    "\n",
    "#         print(f\"Stored chunks from '{json_file}' in '{output_folder}' as text files\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error storing chunks: {e}\")\n",
    "\n",
    "# # Example usage:\n",
    "# json_file = 'icar_crida_reports_processed_pdfs.json'\n",
    "# output_folder = 'icar_crida_reports_chunking_pdfs'\n",
    "\n",
    "# store_chunks_in_text_files(json_file, output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # # Fixed-size chunking\n",
    "# # from langchain.text_splitter import CharacterTextSplitter\n",
    "# # text_splitter = CharacterTextSplitter(\n",
    "# #     separator = \"\\n\\n\",\n",
    "# #     chunk_size = 256,\n",
    "# #     chunk_overlap  = 20\n",
    "# # )\n",
    "# # docs = text_splitter.create_documents([text])\n",
    "# # for doc in docs:\n",
    "# #     print(len(doc.page_content))\n",
    "# # docs\n",
    "# # Import the necessary module from langchain\n",
    "# from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# # Print the original text length for verification\n",
    "# print(\"Original Text Length:\", len(text))\n",
    "# print(\"=\" * 40)\n",
    "\n",
    "# # Initialize the CharacterTextSplitter with the correct parameters\n",
    "# text_splitter = CharacterTextSplitter(\n",
    "#     chunk_size=100,     # Desired chunk size\n",
    "#     chunk_overlap=20    # Overlap between chunks\n",
    "# )\n",
    "\n",
    "# # Create documents (chunks) from the text\n",
    "# docs = text_splitter.create_documents([text])\n",
    "\n",
    "# # Print intermediate results for debugging\n",
    "# print(\"Chunks Created:\", len(docs))\n",
    "# if docs:\n",
    "#     for i, doc in enumerate(docs):\n",
    "#         print(f\"Chunk {i+1} Length: {len(doc.page_content)}\")\n",
    "#         print(doc.page_content)\n",
    "#         print('-' * 40)  # Separator for readability\n",
    "# else:\n",
    "#     print(\"No chunks were created. Please check the text and parameters.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs = text.split(\".\")\n",
    "# # docs\n",
    "# # not effective at all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.text_splitter import NLTKTextSplitter\n",
    "# text_splitter = NLTKTextSplitter(chunk_size = 300, chunk_overlap = 20)\n",
    "# docs = text_splitter.split_text(text)\n",
    "# print(len(docs))\n",
    "# print(len(docs[0]))\n",
    "# docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.text_splitter import SpacyTextSplitter\n",
    "# text_splitter = SpacyTextSplitter()\n",
    "# docs = text_splitter.split_text(text)\n",
    "# docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# text_splitter = RecursiveCharacterTextSplitter(\n",
    "#     # Set a really small chunk size, just to show.\n",
    "#     chunk_size = 256,\n",
    "#     chunk_overlap  = 20\n",
    "# )\n",
    "\n",
    "# docs = text_splitter.create_documents([text])\n",
    "# docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.text_splitter import MarkdownTextSplitter\n",
    "\n",
    "# markdown_splitter = MarkdownTextSplitter(chunk_size=100, chunk_overlap=0)\n",
    "# docs = markdown_splitter.create_documents([text])\n",
    "# docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.text_splitter import LatexTextSplitter\n",
    "# latex_splitter = LatexTextSplitter(chunk_size=100, chunk_overlap=0)\n",
    "# docs = latex_splitter.create_documents([text])\n",
    "# print(len(docs))\n",
    "# docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --quiet langchain_experimental langchain_openai"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
